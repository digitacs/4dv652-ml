{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unauthorized-clone",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unauthorized-clone",
    "outputId": "97f4feb6-ba5c-4d03-c06c-80ba16a8445e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "np.random.seed(47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suited-aspect",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suited-aspect",
    "outputId": "4ea7d50a-2609-45ee-8b98-ca9650f25b97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices( 'GPU' )\n",
    "print( 'Num GPUs Available: ', len( physical_devices ) )\n",
    "if len( physical_devices ) > 0:\n",
    "    tf.config.experimental.set_memory_growth( physical_devices[0], True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "neural-cleveland",
   "metadata": {
    "id": "neural-cleveland"
   },
   "outputs": [],
   "source": [
    "experiment_files = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23', 'A24', 'A25', 'A26', 'A27', 'A28', 'A29', 'A30', 'A31', 'A32', 'A33', 'A34', 'A35', 'A36', 'A37', 'A38', 'A39', 'A40', 'A41', 'A42', 'A43', 'A44', 'A45', 'A46', 'A52', 'A56', 'A61', 'A62', 'A63', 'A64', 'A66', 'A67', 'A68', 'A69', 'A70', 'A71', 'A72', 'A74', 'A81', 'A85', 'A89', 'A91', 'A99', 'A109', 'A110', 'A112', 'A113', 'A114', 'A115', 'A127', 'A129', 'A136', 'A137', 'A138', 'A139', 'A141', 'A142', 'A143', 'A144', 'A145', 'A146', 'A147', 'A148', 'A149', 'A152', 'A153', 'A154', 'A155', 'A156', 'A157', 'A159']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "finished-participation",
   "metadata": {
    "id": "finished-participation"
   },
   "outputs": [],
   "source": [
    "posetnet_dataset_path = \"https://raw.githubusercontent.com/digitacs/4dv652-ml/main/datasets/marked_start_mid_end/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-letters",
   "metadata": {
    "id": "laden-letters"
   },
   "source": [
    "## Reading Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fleet-finding",
   "metadata": {
    "id": "fleet-finding"
   },
   "outputs": [],
   "source": [
    "dataset = None\n",
    "\n",
    "for file in experiment_files:\n",
    "    posenet_data = pd.read_csv(posetnet_dataset_path+'{}.csv'.format( file ))\n",
    "    \n",
    "    if dataset is None:\n",
    "        dataset = posenet_data\n",
    "    else:\n",
    "        dataset = pd.concat((dataset, posenet_data),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incoming-cooler",
   "metadata": {
    "id": "incoming-cooler"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-663a43176429>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['status'][dataset['status']==0] = 'Start'\n"
     ]
    }
   ],
   "source": [
    "dataset['status'][dataset['status']==0] = 'Start'\n",
    "dataset['status'][dataset['status']==1] = 'None'\n",
    "dataset['status'][dataset['status']==2] = 'End'\n",
    "dataset.drop(columns=['FrameNo'], inplace=True)\n",
    "dataset['status'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S8WDTnIBaICq",
   "metadata": {
    "id": "S8WDTnIBaICq"
   },
   "source": [
    "### Examine the class label imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "collected-diabetes",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "collected-diabetes",
    "outputId": "42071fb7-bad6-463d-d1d7-f97c1290aa1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 19417\n",
      "    Start: 4667 (0.24% of total)\n",
      "    End: 4311 (0.22% of total)\n",
      "    None: 10439 (0.54% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_gc = dataset.groupby(['status']).size()\n",
    "print(\n",
    "    'Examples:\\n    Total: {}\\n    Start: {} ({:.2f}% of total)\\n    End: {} ({:.2f}% of total)\\n    None: {} ({:.2f}% of total)\\n'\n",
    "    .format(\n",
    "      len(dataset), \n",
    "      train_gc['Start'],\n",
    "      train_gc['Start'] / len(dataset),\n",
    "      train_gc['End'],\n",
    "      train_gc['End'] / len(dataset),\n",
    "      train_gc['None'],\n",
    "      train_gc['None'] / len(dataset)\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WiRKXqHpaMky",
   "metadata": {
    "id": "WiRKXqHpaMky"
   },
   "source": [
    "### Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "laden-dispatch",
   "metadata": {
    "id": "laden-dispatch"
   },
   "outputs": [],
   "source": [
    "target_data = pd.get_dummies(dataset['status'], prefix='is')\n",
    "input_data = dataset.drop(columns=['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "therapeutic-trustee",
   "metadata": {
    "id": "therapeutic-trustee"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(input_data)\n",
    "input_data = scaler.transform(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-publicity",
   "metadata": {
    "id": "modern-publicity"
   },
   "source": [
    "## Reading Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "enhanced-branch",
   "metadata": {
    "id": "enhanced-branch"
   },
   "outputs": [],
   "source": [
    "test_dataset = None\n",
    "\n",
    "for file_no in range(1,23):\n",
    "    posenet_data = pd.read_csv(posetnet_dataset_path+'B{}.csv'.format( file_no ))\n",
    "    \n",
    "    if dataset is None:\n",
    "        test_dataset = posenet_data\n",
    "    else:\n",
    "        test_dataset = pd.concat((test_dataset, posenet_data),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-underwear",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "proof-judges",
   "metadata": {
    "id": "proof-judges"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-cbad75cf242a>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_dataset['status'][test_dataset['status']==0] = 'Start'\n"
     ]
    }
   ],
   "source": [
    "test_dataset['status'][test_dataset['status']==0] = 'Start'\n",
    "test_dataset['status'][test_dataset['status']==1] = 'None'\n",
    "test_dataset['status'][test_dataset['status']==2] = 'End'\n",
    "test_dataset.drop(columns=['FrameNo'], inplace=True)\n",
    "test_dataset['status'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6i6UT7zmWjQD",
   "metadata": {
    "id": "6i6UT7zmWjQD"
   },
   "source": [
    "### Examine the class label imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "UFViGNsiWk01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFViGNsiWk01",
    "outputId": "7b145321-60c9-4af5-84cf-5b6e7a0059d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 5395\n",
      "    Start: 2102 (0.39% of total)\n",
      "    End: 1405 (0.26% of total)\n",
      "    None: 1888 (0.35% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_gc = test_dataset.groupby(['status']).size()\n",
    "print(\n",
    "    'Examples:\\n    Total: {}\\n    Start: {} ({:.2f}% of total)\\n    End: {} ({:.2f}% of total)\\n    None: {} ({:.2f}% of total)\\n'\n",
    "    .format(\n",
    "      len(test_dataset), \n",
    "      train_gc['Start'],\n",
    "      train_gc['Start'] / len(test_dataset),\n",
    "      train_gc['End'],\n",
    "      train_gc['End'] / len(test_dataset),\n",
    "      train_gc['None'],\n",
    "      train_gc['None'] / len(test_dataset)\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t-CATaUNaT2T",
   "metadata": {
    "id": "t-CATaUNaT2T"
   },
   "source": [
    "### Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "confused-broadway",
   "metadata": {
    "id": "confused-broadway"
   },
   "outputs": [],
   "source": [
    "test_target_data = pd.get_dummies(test_dataset['status'], prefix='is')\n",
    "test_input_data = test_dataset.drop(columns=['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "supreme-suite",
   "metadata": {
    "id": "supreme-suite"
   },
   "outputs": [],
   "source": [
    "test_input_data = scaler.transform(test_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-first",
   "metadata": {
    "id": "subjective-first"
   },
   "source": [
    "## Create a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3KI45mwNecVN",
   "metadata": {
    "id": "3KI45mwNecVN"
   },
   "source": [
    "### The model **configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3nOFg7Geco6",
   "metadata": {
    "id": "a3nOFg7Geco6"
   },
   "outputs": [],
   "source": [
    "from keras.metrics import TruePositives,FalsePositives,TrueNegatives,FalseNegatives,CategoricalAccuracy,Precision,Recall,AUC\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "METRICS = [\n",
    "      TruePositives(name='tp'),\n",
    "      FalsePositives(name='fp'),\n",
    "      TrueNegatives(name='tn'),\n",
    "      FalseNegatives(name='fn'), \n",
    "      CategoricalAccuracy(name='accuracy'),\n",
    "      Precision(name='precision'),\n",
    "      Recall(name='recall'),\n",
    "      AUC(name='acc'),\n",
    "      AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "TEST_ID = 38\n",
    "ACTIVATION = 'exponential'\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 64\n",
    "OPTIMIZER = 'rmsprop'\n",
    "LOSS = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_prc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RYzrdbATwnFK",
   "metadata": {
    "id": "RYzrdbATwnFK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "impressive-diversity",
   "metadata": {
    "id": "impressive-diversity"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, BatchNormalization, MaxPooling2D, Flatten\n",
    "\n",
    "def make_model(optimizer = OPTIMIZER ,loss = LOSS , metrics = METRICS, output_size=3):\n",
    "  model = Sequential()\n",
    "  model.add(Dense( units=26, input_dim=26, activation='relu' ))\n",
    "  model.add(Dense(32, activation = ACTIVATION, kernel_initializer = 'he_uniform'))\n",
    "  model.add(Dense(32, activation = ACTIVATION, kernel_initializer = 'he_uniform'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(output_size, activation = 'softmax'))\n",
    "\n",
    "\n",
    "  # Todo: check other optimizer like 'adam' and 'nadam' as well\n",
    "  model.compile(optimizer = optimizer ,loss = loss, metrics = metrics)\n",
    "  print(model.summary())\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mysterious-benefit",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mysterious-benefit",
    "outputId": "ed9af563-5f90-4270-9655-1901e67474d3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 26)                702       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                864       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/dense_3/Softmax:0) = ] [[-nan -nan -nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]] [Op:__inference_train_function_3369]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1518a4026e7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = base_model.fit( \n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    898\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/dense_3/Softmax:0) = ] [[-nan -nan -nan]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]] [Op:__inference_train_function_3369]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "base_model = make_model()\n",
    "\n",
    "history = base_model.fit( \n",
    "    x=input_data, \n",
    "    y=target_data, \n",
    "    validation_split=0.1, \n",
    "    shuffle=True, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-truth",
   "metadata": {
    "id": "upper-truth"
   },
   "outputs": [],
   "source": [
    "training_history = pd.DataFrame(history.history)\n",
    "training_history['epochs'] = range(len(training_history['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-cooler",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "id": "closing-cooler",
    "outputId": "bf9ef4ae-e8d6-4a84-bc34-c2f27aba2a14"
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.color_palette(\"husl\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iliUzQxgf34G",
   "metadata": {
    "id": "iliUzQxgf34G"
   },
   "source": [
    "### Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dSYWzGfhf8kR",
   "metadata": {
    "id": "dSYWzGfhf8kR"
   },
   "outputs": [],
   "source": [
    "PLOTS = [\n",
    "           {'cols':['tp', 'val_tp'], 'title':'Training and validation TruePositives', 'yLabel':'TruePositives'},\n",
    "           {'cols':['fp', 'val_fp'], 'title':'Training and validation FalsePositives', 'yLabel':'FalsePositives'},\n",
    "           {'cols':['tn', 'val_tn'], 'title':'Training and validation TrueNegatives', 'yLabel':'TrueNegatives'},\n",
    "           {'cols':['fn', 'val_fn'], 'title':'Training and validation FalseNegatives', 'yLabel':'FalseNegatives'},\n",
    "           {'cols':['accuracy', 'val_accuracy'], 'title':'Training and validation CategoricalAccuracy', 'yLabel':'CategoricalAccuracy'},\n",
    "           {'cols':['precision', 'val_precision'], 'title':'Training and validation Precision', 'yLabel':'Precision'},\n",
    "           {'cols':['recall', 'val_recall'], 'title':'Training and validation Recall', 'yLabel':'Recall'},\n",
    "           {'cols':['acc', 'val_acc'], 'title':'Training and validation AUC', 'yLabel':'AUC'},\n",
    "           {'cols':['prc', 'val_prc'], 'title':'Training and validation PRC', 'yLabel':'PRC'} # precision-recall curve\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XHzWlrt4eWtP",
   "metadata": {
    "id": "XHzWlrt4eWtP"
   },
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yHrdmEmVj4C-",
   "metadata": {
    "id": "yHrdmEmVj4C-"
   },
   "source": [
    "### Overal Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-thousand",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ongoing-thousand",
    "outputId": "9298a737-015b-4370-fc00-5cf656bca3dd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = base_model.predict( x=test_input_data, batch_size=BATCH_SIZE, verbose=1 )\n",
    "predictions = pd.DataFrame(predictions.round().astype(int), columns=['is_End',\t'is_None',\t'is_Start'])\n",
    "\n",
    "results = { \n",
    "    'name': 'Sigmoid',\n",
    "    'accuracy score': accuracy_score( test_target_data, predictions ),\n",
    "}\n",
    "\n",
    "acc_a = 'Accuracy: {:.2f}%'.format( results['accuracy score']*100 ) \n",
    "print(acc_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T6mq9Tv9j7_E",
   "metadata": {
    "id": "T6mq9Tv9j7_E"
   },
   "source": [
    "### Start and End Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2y8ePcpgWZEv",
   "metadata": {
    "id": "2y8ePcpgWZEv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RQvqpbnLj_Ph",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQvqpbnLj_Ph",
    "outputId": "f5921147-77e8-4308-e606-c1900a118949"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = base_model.predict( x=test_input_data, batch_size=BATCH_SIZE, verbose=1 )\n",
    "predictions = pd.DataFrame(predictions.round().astype(int), columns=['is_End',\t'is_None',\t'is_Start'])\n",
    "bool_test_not_none = np.array(test_target_data['is_None']) != 1\n",
    "y_prod = pd.DataFrame(predictions[ bool_test_not_none])\n",
    "y_true = pd.DataFrame(test_target_data[ bool_test_not_none])\n",
    "\n",
    "results = { \n",
    "    'name': 'Sigmoid',\n",
    "    'accuracy score': accuracy_score( y_true, y_prod ),\n",
    "}\n",
    "\n",
    "acc_b = 'Accuracy: {:.2f}%'.format( results['accuracy score']*100 ) \n",
    "print(acc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dSpTPbYIg8HR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dSpTPbYIg8HR",
    "outputId": "d7374746-8a83-4e8d-8821-7f7f7e6779c1"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "fig_no = 331\n",
    "model_type = 'Base Model'\n",
    "title = 'Model #{} ({})\\n Overal {}  -  Strat.Stop {}\\n Activation function: {}\\n Numner of epochs: {} / Batch size: {}\\nOptimizer: {} / Loss: {}'.format(TEST_ID, model_type,acc_a,acc_b, ACTIVATION, EPOCHS,BATCH_SIZE,OPTIMIZER,LOSS)\n",
    "fig.suptitle(title, fontsize=\"x-large\")\n",
    "\n",
    "for i in range(len(PLOTS)):\n",
    "    ax = fig.add_subplot(str(fig_no))\n",
    "    sns.lineplot(data=training_history[PLOTS[i]['cols']], ax=ax)\n",
    "    plt.title(PLOTS[i]['title'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(PLOTS[i]['yLabel'])\n",
    "    fig_no += 1\n",
    "    \n",
    "plt.savefig('Model-{}({})'.format(TEST_ID, model_type))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l7qMa5hJnVKb",
   "metadata": {
    "id": "l7qMa5hJnVKb"
   },
   "source": [
    "# Solution-1: Class weighting\n",
    "These will cause the model to \"pay more attention\" to examples from an under-represented class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mVHtoT9PnZCz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVHtoT9PnZCz",
    "outputId": "f88c8f1e-fdb3-4487-dd98-23862941168d"
   },
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "train_gc = dataset.groupby(['status']).size()\n",
    "total = len(dataset)\n",
    "\n",
    "weight_for_Start = (1 / train_gc['Start'])*(total)/3.0 \n",
    "weight_for_End = (1 / train_gc['End'])*(total)/3.0\n",
    "weight_for_None = (1 / train_gc['None'])*(total)/3.0\n",
    "\n",
    "class_weight = {0: weight_for_Start, 1: weight_for_End, 2:weight_for_None}\n",
    "\n",
    "print('Weight for class Start: {:.2f}'.format(weight_for_Start))\n",
    "print('Weight for class End: {:.2f}'.format(weight_for_End))\n",
    "print('Weight for class None: {:.2f}'.format(weight_for_None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QRDoZHXNoTXA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRDoZHXNoTXA",
    "outputId": "cf0b5a73-54ea-40a5-fa9c-879e87066d65"
   },
   "outputs": [],
   "source": [
    "weighted_model = make_model()\n",
    "\n",
    "weighted_history = weighted_model.fit(\n",
    "    x=input_data, \n",
    "    y=target_data, \n",
    "    validation_split=0.1, \n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2,\n",
    "    # The class weights go here\n",
    "    class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d-Gtc4ZRp-8o",
   "metadata": {
    "id": "d-Gtc4ZRp-8o"
   },
   "source": [
    "### Overal Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjCT2HtLp-8p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjCT2HtLp-8p",
    "outputId": "98886169-77f2-47d9-8854-eb8e907527cc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = weighted_model.predict( x=test_input_data, batch_size=BATCH_SIZE, verbose=1 )\n",
    "predictions = pd.DataFrame(predictions.round().astype(int), columns=['is_End',\t'is_None',\t'is_Start'])\n",
    "\n",
    "results = { \n",
    "    'name': 'Sigmoid',\n",
    "    'accuracy score': accuracy_score( test_target_data, predictions ),\n",
    "}\n",
    "\n",
    "acc_a = 'Accuracy: {:.2f}%'.format( results['accuracy score']*100 ) \n",
    "print(acc_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z8W9vdiMp-8q",
   "metadata": {
    "id": "z8W9vdiMp-8q"
   },
   "source": [
    "### Start and End Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AKFHfr6Zp-8r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKFHfr6Zp-8r",
    "outputId": "f943e02b-d827-46ab-bf8c-25dd95cf68be"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = weighted_model.predict( x=test_input_data, batch_size=BATCH_SIZE, verbose=1 )\n",
    "predictions = pd.DataFrame(predictions.round().astype(int), columns=['is_End',\t'is_None',\t'is_Start'])\n",
    "bool_test_not_none = np.array(test_target_data['is_None']) != 1\n",
    "y_prod = pd.DataFrame(predictions[ bool_test_not_none])\n",
    "y_true = pd.DataFrame(test_target_data[ bool_test_not_none])\n",
    "\n",
    "results = { \n",
    "    'name': 'Sigmoid',\n",
    "    'accuracy score': accuracy_score( y_true, y_prod ),\n",
    "}\n",
    "\n",
    "acc_b = 'Accuracy: {:.2f}%'.format( results['accuracy score']*100 ) \n",
    "print(acc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oACKXHXzpQZ4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oACKXHXzpQZ4",
    "outputId": "f579c6fe-056f-4e46-bd31-dd6b6710890c"
   },
   "outputs": [],
   "source": [
    "weighted_history = pd.DataFrame(weighted_history.history)\n",
    "weighted_history['epochs'] = range(len(weighted_history['accuracy']))\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig_no = 331\n",
    "model_type = 'Class Weighting'\n",
    "title = 'Model #{} ({})\\n Overal {}  -  Strat.Stop {}\\n Activation function: {}\\n Numner of epochs: {} / Batch size: {}\\nOptimizer: {} / Loss: {}'.format(TEST_ID, model_type,acc_a,acc_b, ACTIVATION, EPOCHS,BATCH_SIZE,OPTIMIZER,LOSS)\n",
    "fig.suptitle(title, fontsize=\"x-large\")\n",
    "\n",
    "for i in range(len(PLOTS)):\n",
    "    ax = fig.add_subplot(str(fig_no))\n",
    "    sns.lineplot(data=training_history[PLOTS[i]['cols']], ax=ax)\n",
    "    plt.title(PLOTS[i]['title'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(PLOTS[i]['yLabel'])\n",
    "    fig_no += 1\n",
    "    \n",
    "plt.savefig('Model-{}({})'.format(TEST_ID, model_type))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VcmaWkYiqQwJ",
   "metadata": {
    "id": "VcmaWkYiqQwJ"
   },
   "source": [
    "#  Solution-2: Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EJt5ZpATqgRi",
   "metadata": {
    "id": "EJt5ZpATqgRi"
   },
   "outputs": [],
   "source": [
    "bool_train_not_none = np.array(target_data['is_None']) != 1\n",
    "\n",
    "pos_features = input_data[bool_train_not_none]\n",
    "neg_features = input_data[~bool_train_not_none]\n",
    "\n",
    "pos_labels = target_data[bool_train_not_none]\n",
    "neg_labels = target_data[~bool_train_not_none]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gov4rODAbjRK",
   "metadata": {
    "id": "gov4rODAbjRK"
   },
   "source": [
    "Fill choices array randomly utill it become the same size as negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q0-0u7wpV1YC",
   "metadata": {
    "id": "Q0-0u7wpV1YC"
   },
   "outputs": [],
   "source": [
    "ids = np.arange(len(pos_features))\n",
    "choices = np.random.choice(ids, len(neg_features))\n",
    "\n",
    "res_pos_features = pos_features[choices]\n",
    "res_pos_labels = pos_labels.iloc[choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30xV4bW3a3lB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30xV4bW3a3lB",
    "outputId": "4cecb4b7-9ec1-43a9-a772-0f1229294d70"
   },
   "outputs": [],
   "source": [
    "resampled_features = np.concatenate([res_pos_features, neg_features], axis=0)\n",
    "resampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n",
    "\n",
    "order = np.arange(len(resampled_labels))\n",
    "np.random.shuffle(order)\n",
    "resampled_features = resampled_features[order]\n",
    "resampled_labels = resampled_labels[order]\n",
    "\n",
    "resampled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mN_cAsWBdIS2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mN_cAsWBdIS2",
    "outputId": "9cf68127-261f-4c65-fb82-7c6c88366154"
   },
   "outputs": [],
   "source": [
    "resampled_model = make_model()\n",
    "\n",
    "resampled_history = resampled_model.fit(\n",
    "    x=resampled_features,\n",
    "    y=resampled_labels,\n",
    "    validation_split=0.1, \n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jTnnh59KeGK9",
   "metadata": {
    "id": "jTnnh59KeGK9"
   },
   "source": [
    "### Overal Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TjcHyyZqeGK9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjcHyyZqeGK9",
    "outputId": "a9044ad0-3f33-4809-eb7b-1566eceb0f50",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = resampled_model.predict( x=test_input_data, batch_size=BATCH_SIZE, verbose=1 )\n",
    "predictions = pd.DataFrame(predictions.round().astype(int), columns=['is_End',\t'is_None',\t'is_Start'])\n",
    "\n",
    "results = { \n",
    "    'name': 'Sigmoid',\n",
    "    'accuracy score': accuracy_score( test_target_data, predictions ),\n",
    "}\n",
    "\n",
    "acc_a = 'Accuracy: {:.2f}%'.format( results['accuracy score']*100 ) \n",
    "print(acc_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uJdOnBYyeGK-",
   "metadata": {
    "id": "uJdOnBYyeGK-"
   },
   "source": [
    "### Start and End Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tnI0GNbHeGK-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnI0GNbHeGK-",
    "outputId": "929c7b58-1f87-4968-99ca-36067dd184ce"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = resampled_model.predict( x=test_input_data, batch_size=BATCH_SIZE, verbose=1 )\n",
    "predictions = pd.DataFrame(predictions.round().astype(int), columns=['is_End',\t'is_None',\t'is_Start'])\n",
    "bool_test_not_none = np.array(test_target_data['is_None']) != 1\n",
    "y_prod = pd.DataFrame(predictions[ bool_test_not_none])\n",
    "y_true = pd.DataFrame(test_target_data[ bool_test_not_none])\n",
    "\n",
    "results = { \n",
    "    'name': 'Sigmoid',\n",
    "    'accuracy score': accuracy_score( y_true, y_prod ),\n",
    "}\n",
    "\n",
    "acc_b = 'Accuracy: {:.2f}%'.format( results['accuracy score']*100 ) \n",
    "print(acc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ppbCIN9_eBAC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ppbCIN9_eBAC",
    "outputId": "cb65733f-a60a-4753-f3d4-e539a8fe7d10"
   },
   "outputs": [],
   "source": [
    "resampled_history = pd.DataFrame(resampled_history.history)\n",
    "resampled_history['epochs'] = range(len(resampled_history['accuracy']))\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig_no = 331\n",
    "model_type = 'Over Sampling'\n",
    "title = 'Model #{} ({})\\n Overal {}  -  Strat.Stop {}\\n Activation function: {}\\n Numner of epochs: {} / Batch size: {}\\nOptimizer: {} / Loss: {}'.format(TEST_ID, model_type,acc_a,acc_b, ACTIVATION, EPOCHS,BATCH_SIZE,OPTIMIZER,LOSS)\n",
    "fig.suptitle(title, fontsize=\"x-large\")\n",
    "\n",
    "for i in range(len(PLOTS)):\n",
    "    ax = fig.add_subplot(str(fig_no))\n",
    "    sns.lineplot(data=training_history[PLOTS[i]['cols']], ax=ax)\n",
    "    plt.title(PLOTS[i]['title'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(PLOTS[i]['yLabel'])\n",
    "    fig_no += 1\n",
    "    \n",
    "plt.savefig('Model-{}({})'.format(TEST_ID, model_type))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81if4zTiv_vg",
   "metadata": {
    "id": "81if4zTiv_vg"
   },
   "source": [
    "#  Solution-3: Focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jQHkFCTrueGu",
   "metadata": {
    "id": "jQHkFCTrueGu"
   },
   "outputs": [],
   "source": [
    "target_data_focal = target_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XOS-215bvIAu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOS-215bvIAu",
    "outputId": "3a97fa33-e8aa-45ea-a1b9-24bac7ec2258"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def categorical_focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Implementation of Focal Loss from the paper in multiclass classification\n",
    "    Formula:\n",
    "        loss = -alpha*((1-p)^gamma)*log(p)\n",
    "    Parameters:\n",
    "        alpha -- the same as wighting factor in balanced cross entropy\n",
    "        gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "        gamma -- 2.0 as mentioned in the paper\n",
    "        alpha -- 0.25 as mentioned in the paper\n",
    "    \"\"\"\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Define epsilon so that the backpropagation will not result in NaN\n",
    "        # for 0 divisor case\n",
    "        epsilon = K.epsilon()\n",
    "        # Add the epsilon to prediction value\n",
    "        #y_pred = y_pred + epsilon\n",
    "        # Clip the prediction value\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -y_true*K.log(y_pred)\n",
    "        # Calculate weight that consists of  modulating factor and weighting factor\n",
    "        weight = alpha * y_true * K.pow((1-y_pred), gamma)\n",
    "        # Calculate focal loss\n",
    "        loss = weight * cross_entropy\n",
    "        # Sum the losses in mini_batch\n",
    "        loss = K.sum(loss, axis=1)\n",
    "        return loss\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "focal_loss_model = make_model(loss=categorical_focal_loss(gamma=3))\n",
    "\n",
    "focal_loss_history = focal_loss_model.fit( \n",
    "    x=input_data, \n",
    "    y=target_data_focal, \n",
    "    validation_split=0.1, \n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jJfjx5aBwd9A",
   "metadata": {
    "id": "jJfjx5aBwd9A"
   },
   "source": [
    "### Overal Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hexL2-A3wd9A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hexL2-A3wd9A",
    "outputId": "961ceedc-1100-46dc-c1f5-f5c10d7dc89b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = focal_loss_model.predict( x=test_input_data, batch_size=BATCH_SIZE, verbose=1 )\n",
    "predictions = pd.DataFrame(predictions.round().astype(int), columns=['is_End',\t'is_None',\t'is_Start'])\n",
    "results = { \n",
    "    'name': 'Sigmoid',\n",
    "    'accuracy score': accuracy_score( test_target_data, predictions ),\n",
    "}\n",
    "\n",
    "acc_a = 'Accuracy: {:.2f}%'.format( results['accuracy score']*100 ) \n",
    "print(acc_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UPyWFR93wd9B",
   "metadata": {
    "id": "UPyWFR93wd9B"
   },
   "source": [
    "### Start and End Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JBYg1EW8wd9C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JBYg1EW8wd9C",
    "outputId": "7cb57126-79ae-4e26-c24f-32d80a555fb9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = focal_loss_model.predict( x=test_input_data, batch_size=BATCH_SIZE, verbose=1 )\n",
    "predictions = pd.DataFrame(predictions.round().astype(int), columns=['is_End',\t'is_None',\t'is_Start'])\n",
    "bool_test_not_none = np.array(test_target_data['is_None']) != 1\n",
    "y_prod = pd.DataFrame(predictions[ bool_test_not_none])\n",
    "y_true = pd.DataFrame(test_target_data[ bool_test_not_none])\n",
    "\n",
    "results = { \n",
    "    'name': 'Sigmoid',\n",
    "    'accuracy score': accuracy_score( y_true, y_prod ),\n",
    "}\n",
    "\n",
    "acc_b = 'Accuracy: {:.2f}%'.format( results['accuracy score']*100 ) \n",
    "print(acc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BlMHVTZIwQWl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BlMHVTZIwQWl",
    "outputId": "35a4454a-f2fb-4f75-86fc-cb36a2c2cdeb"
   },
   "outputs": [],
   "source": [
    "focal_loss_history = pd.DataFrame(focal_loss_history.history)\n",
    "focal_loss_history['epochs'] = range(len(focal_loss_history['accuracy']))\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig_no = 331\n",
    "model_type = 'Focal Loss'\n",
    "title = 'Model #{} ({})\\n Overal {}  -  Strat.Stop {}\\n Activation function: {}\\n Numner of epochs: {} / Batch size: {}\\nOptimizer: {} / Loss: {}'.format(TEST_ID, model_type,acc_a,acc_b, ACTIVATION, EPOCHS,BATCH_SIZE,OPTIMIZER,LOSS)\n",
    "fig.suptitle(title, fontsize=\"x-large\")\n",
    "\n",
    "for i in range(len(PLOTS)):\n",
    "    ax = fig.add_subplot(str(fig_no))\n",
    "    sns.lineplot(data=focal_loss_history[PLOTS[i]['cols']], ax=ax)\n",
    "    plt.title(PLOTS[i]['title'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(PLOTS[i]['yLabel'])\n",
    "    fig_no += 1\n",
    "    \n",
    "plt.savefig('Model-{}({})'.format(TEST_ID, model_type))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Multi class classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
