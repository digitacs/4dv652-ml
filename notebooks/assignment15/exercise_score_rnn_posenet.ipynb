{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intermediate-client",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secret-combining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# General requirements\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# MLflow dashboard\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://35.228.45.76:5000')\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='../../keys/mlflow-312506-8cfad529f4fd.json'\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Import data augmentation\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from augmentation.methods import *\n",
    "\n",
    "# Set random state\n",
    "import numpy as np\n",
    "random_state = 47\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# Use GPU (if available)\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices( 'GPU' )\n",
    "print( 'Num GPUs Available: ', len( physical_devices ) )\n",
    "if len( physical_devices ) > 0:\n",
    "    tf.config.experimental.set_memory_growth( physical_devices[0], True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-parent",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n",
    "\n",
    "## 1.1 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conventional-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "posenet_dataset_path = '../../datasets/posenet-uncut/'\n",
    "kinect_dataset_path = '../../datasets/kinect_fixed_not_cut/'\n",
    "cut_dataset_path = '../../datasets/new_posenet_marked_start_end/'\n",
    "goodness_score = pd.read_csv('../../datasets/VideoScoring.csv')\n",
    "exrecise_score = pd.read_csv('../../datasets/ExerciseScoring.csv')\n",
    "\n",
    "train_test_ratio = 0.9\n",
    "new_label = 'ExreciseScore'\n",
    "exersice_score_indicator = 'O_Score'\n",
    "goodness_score_indicator = 'AVG'\n",
    "goodness_score_threshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surrounded-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop video augmentation results\n",
    "goodness_score = goodness_score[goodness_score['FileName'].str.match('U')==False]\n",
    "goodness_score = goodness_score[goodness_score[goodness_score_indicator]>=goodness_score_threshold]\n",
    "\n",
    "posenet_ok = goodness_score['FileName']+'.csv'\n",
    "\n",
    "exrecise_score['Posenet'] = exrecise_score['Posenet'] + '.csv'\n",
    "exrecise_score['Kinect'] = exrecise_score['Kinect'] + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "simplified-segment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posenet datasets: 157\n",
      "Total kinect datasets: 157\n"
     ]
    }
   ],
   "source": [
    "posenet_files = []\n",
    "kinect_files = []\n",
    "\n",
    "for file in os.listdir(posenet_dataset_path):\n",
    "    if not file.find(\".csv\",0) == -1:\n",
    "        if file in (goodness_score['FileName']+'.csv').to_list():\n",
    "            posenet_files.append(file)\n",
    "        \n",
    "for file in os.listdir(kinect_dataset_path):\n",
    "    if not file.find(\".csv\",0) == -1:\n",
    "        if file in (goodness_score['FileName']+'_kinect.csv').to_list():\n",
    "            kinect_files.append(file)\n",
    "        \n",
    "print('Total posenet datasets: {}'.format(len(posenet_files)))\n",
    "print('Total kinect datasets: {}'.format(len(kinect_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unnecessary-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 141\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SPLIT_POSENET = int(len(posenet_files)*train_test_ratio)\n",
    "TRAIN_SPLIT_KINECT = int(len(kinect_files)*train_test_ratio)\n",
    "print(TRAIN_SPLIT_POSENET,TRAIN_SPLIT_KINECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-prior",
   "metadata": {},
   "source": [
    "## 1.2 Reshaping the datasets for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "romance-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(df):\n",
    "    augmented_datasets = []\n",
    "    \n",
    "    # Add original data\n",
    "    augmented_datasets.append(df)\n",
    "    \n",
    "    # Mirror X coordinate\n",
    "    for i in range(len(augmented_datasets)):\n",
    "        augmented_datasets.append(mirror(augmented_datasets[i],'x', append=False))\n",
    "\n",
    "    # Stretch by 50%\n",
    "    for i in range(len(augmented_datasets)):\n",
    "        df_temp = augMultiplier(augmented_datasets[i].drop(columns=[new_label]), multiplier=1.5)\n",
    "        df_temp[new_label] = augmented_datasets[i][new_label]\n",
    "        augmented_datasets.append(df_temp)\n",
    "    \n",
    "\n",
    "    # Compress by 25%\n",
    "    for i in range(len(augmented_datasets)):\n",
    "        df_temp = augMultiplier(augmented_datasets[i].drop(columns=[new_label]), multiplier=0.25)\n",
    "        df_temp[new_label] = augmented_datasets[i][new_label]\n",
    "        augmented_datasets.append(df_temp)\n",
    "   \n",
    "\n",
    "    # Rotate by p/7\n",
    "#     samples = df.sample(5000)\n",
    "#     angle = 3.1415 / 7\n",
    "#     samples_rotated = rotate(samples.drop(columns=[new_label]), angle=angle, posenet=False)\n",
    "#     samples_rotated[new_label] = samples[new_label].append(samples[new_label], ignore_index=True)\n",
    "#     df = df.append(samples_rotated, ignore_index=True)\n",
    "#     print(df.shape)\n",
    "\n",
    "    # Rotate by -p/9\n",
    "#     samples = df.sample(5000)\n",
    "#     angle = 3.1415 / -9\n",
    "#     samples_rotated = rotate(samples.drop(columns=[new_label]), angle=angle, posenet=False)\n",
    "#     samples_rotated[new_label] = samples[new_label].append(samples[new_label], ignore_index=True)\n",
    "#     df = df.append(samples_rotated, ignore_index=True)\n",
    "#     print(df.shape)\n",
    "\n",
    "    return augmented_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adopted-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY_SIZE = 100\n",
    "STEP = 3\n",
    "N_FEATURES = 26\n",
    "\n",
    "def multivariate_data(dataset, target_column, history_columns):\n",
    "\n",
    "    segments = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(0,len(dataset) - HISTORY_SIZE, STEP):\n",
    "        segments.append(dataset[history_columns].values[i:i + HISTORY_SIZE])\n",
    "        labels.append(dataset[target_column].mean())\n",
    "    \n",
    "    return np.array(segments), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hungarian-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_columns = ['head_x', 'head_y', 'left_shoulder_x', 'left_shoulder_y',\n",
    "       'right_shoulder_x', 'right_shoulder_y', 'left_elbow_x', 'left_elbow_y',\n",
    "       'right_elbow_x', 'right_elbow_y', 'left_wrist_x', 'left_wrist_y',\n",
    "       'right_wrist_x', 'right_wrist_y', 'left_hip_x', 'left_hip_y',\n",
    "       'right_hip_x', 'right_hip_y', 'left_knee_x', 'left_knee_y',\n",
    "       'right_knee_x', 'right_knee_y', 'left_ankle_x', 'left_ankle_y',\n",
    "       'right_ankle_x', 'right_ankle_y']\n",
    "target_column = 'ExreciseScore'\n",
    "\n",
    "def read_dataset(path,file_list,split_point,is_posenet=True,isTrain=True):\n",
    "    all_segments = None\n",
    "    all_labels = None\n",
    "    \n",
    "    start=0\n",
    "    end=0\n",
    "    \n",
    "    if isTrain:\n",
    "        start = 0\n",
    "        end = split_point\n",
    "    else:\n",
    "        start = split_point\n",
    "        end = len(file_list)\n",
    "    \n",
    "    for file in file_list[start:end]:\n",
    "        df = pd.read_csv(path+file)\n",
    "        \n",
    "        if is_posenet:  \n",
    "            df = df[df.columns.drop(list(df.filter(regex='_eye_')))]\n",
    "            df = df[df.columns.drop(list(df.filter(regex='_ear_')))]\n",
    "            df = df[df.columns.drop(list(df.filter(regex='score')))]\n",
    "            df = df.rename(columns={'nose_x': 'head_x', 'nose_y': 'head_y'})\n",
    "            df[new_label] = float(exrecise_score[exrecise_score['Posenet'] == file][exersice_score_indicator])\n",
    "        else:  \n",
    "            df = df.drop(columns=['Unnamed: 0','FrameNo'])\n",
    "            df[new_label] = float(exrecise_score[exrecise_score['Kinect'] == file][exersice_score_indicator])\n",
    "            \n",
    "        # Cut start / end\n",
    "        try:\n",
    "            se = pd.read_csv('../../datasets/new_posenet_marked_start_end/'+file)\n",
    "            cut_start = min(se[(se['start']==0) * (se['end']==0)].index)\n",
    "            cut_end = max(se[(se['start']==0) * (se['end']==0)].index)\n",
    "            df = df.iloc[cut_start:cut_end]\n",
    "        except IOError as e:\n",
    "            print('Error in reading file: ', e)\n",
    "            \n",
    "    \n",
    "        if isTrain:\n",
    "            for sets in data_augmentation(df):\n",
    "                segment,label = multivariate_data(sets,target_column,history_columns)\n",
    "\n",
    "                if all_segments is None:\n",
    "                    all_segments = segment\n",
    "                    all_labels = label\n",
    "                else:\n",
    "                    all_segments = np.append(all_segments,segment,axis=0)  \n",
    "                    all_labels = np.append(all_labels,label,axis=0)  \n",
    "\n",
    "                sys.stdout.write(\"Total added rows: %d   \\r\" % (all_segments.shape[0]) )\n",
    "                sys.stdout.flush()\n",
    "        else:\n",
    "            segment,label = multivariate_data(df,target_column,history_columns)\n",
    "\n",
    "            if all_segments is None:\n",
    "                all_segments = segment\n",
    "                all_labels = label\n",
    "            else:\n",
    "                all_segments = np.append(all_segments,segment,axis=0)  \n",
    "                all_labels = np.append(all_labels,label,axis=0)\n",
    "    \n",
    "    return all_segments,all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "british-advantage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in reading file:  [Errno 2] No such file or directory: '../../datasets/new_posenet_marked_start_end/A60.csv'\n",
      "Total added rows: 152424   \r"
     ]
    }
   ],
   "source": [
    "X_train, y_train = read_dataset(posenet_dataset_path,posenet_files,TRAIN_SPLIT_POSENET,True,True)\n",
    "X_test, y_test = read_dataset(posenet_dataset_path,posenet_files,TRAIN_SPLIT_POSENET,True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "technological-hebrew",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152424, 100, 26)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "apart-brunswick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1702, 100, 26)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-characteristic",
   "metadata": {},
   "source": [
    "## 1.3 Standardize features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wooden-university",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape\n",
      "(152424, 100, 26)\n",
      "(152424,)\n",
      "Test data shape\n",
      "(1702, 100, 26)\n",
      "(1702,)\n"
     ]
    }
   ],
   "source": [
    "train_mean = X_train.mean()\n",
    "train_std = X_train.std()\n",
    "X_train = (X_train-train_mean)/train_std\n",
    "X_test = (X_test-train_mean)/train_std\n",
    "\n",
    "target_mean = y_train.mean()\n",
    "target_std = y_train.std()\n",
    "y_train = (y_train-target_mean)/target_std\n",
    "y_test = (y_test-target_mean)/target_std\n",
    "\n",
    "\n",
    "print ('Train data shape')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print ('Test data shape')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aggregate-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 60\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_set = train_set.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_set = test_set.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-service",
   "metadata": {},
   "source": [
    "# 2. Define Model Architecture\n",
    "\n",
    "## 2.1 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "excellent-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError, RootMeanSquaredError\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "\n",
    "INPUT_SHAPE = X_train.shape[-2:]\n",
    "\n",
    "metrics = [\n",
    "    MeanSquaredError(name=\"mse\", dtype=None),\n",
    "    MeanAbsoluteError(name=\"mae\", dtype=None),\n",
    "    RootMeanSquaredError(name=\"rmse\", dtype=None),\n",
    "]\n",
    "\n",
    "def create_model(layers, optimizer):\n",
    "    model = tf.keras.models.Sequential(\n",
    "        InputLayer(input_shape=INPUT_SHAPE)\n",
    "    )\n",
    "\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=metrics)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "promising-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    verbose=1,\n",
    "    patience=25,\n",
    "    mode='min',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bronze-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "input_schema = Schema([\n",
    "    ColSpec(\"double\", \"head_x\"),\n",
    "    ColSpec(\"double\", \"head_y\"),\n",
    "    ColSpec(\"double\", \"left_shoulder_x\"),\n",
    "    ColSpec(\"double\", \"left_shoulder_y\"),\n",
    "    ColSpec(\"double\", \"right_shoulder_x\"),\n",
    "    ColSpec(\"double\", \"right_shoulder_y\"),\n",
    "    ColSpec(\"double\", \"left_elbow_x\"),\n",
    "    ColSpec(\"double\", \"left_elbow_y\"),\n",
    "    ColSpec(\"double\", \"right_elbow_x\"),\n",
    "    ColSpec(\"double\", \"right_elbow_y\"),\n",
    "    ColSpec(\"double\", \"left_wrist_x\"),\n",
    "    ColSpec(\"double\", \"left_wrist_y\"),\n",
    "    ColSpec(\"double\", \"right_wrist_x\"),\n",
    "    ColSpec(\"double\", \"right_wrist_y\"),\n",
    "    ColSpec(\"double\", \"left_hip_x\"),\n",
    "    ColSpec(\"double\", \"left_hip_y\"),\n",
    "    ColSpec(\"double\", \"right_hip_x\"),\n",
    "    ColSpec(\"double\", \"right_hip_y\"),\n",
    "    ColSpec(\"double\", \"left_knee_x\"),\n",
    "    ColSpec(\"double\", \"left_knee_y\"),\n",
    "    ColSpec(\"double\", \"right_knee_x\"),\n",
    "    ColSpec(\"double\", \"right_knee_y\"),\n",
    "    ColSpec(\"double\", \"left_ankle_x\"),\n",
    "    ColSpec(\"double\", \"left_ankle_y\"),\n",
    "    ColSpec(\"double\", \"right_ankle_x\"),\n",
    "    ColSpec(\"double\", \"right_ankle_y\"),\n",
    "])\n",
    "output_schema = Schema([\n",
    "    ColSpec(\"double\", \"ExreciseScore\"),\n",
    "])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "impressive-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "def plot_train_history(history, title):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stuck-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    mse = mean_squared_error(actual, pred)\n",
    "    msa = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    variance = explained_variance_score(actual, pred)\n",
    "    return mse, msa, r2, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-duplicate",
   "metadata": {},
   "source": [
    "# 3. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "plastic-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense,GRU,Dropout\n",
    "\n",
    "model_name = 'scoring_rnn_gru_n'\n",
    "\n",
    "OPTIMIZER='adam'\n",
    "EVALUATION_INTERVAL = 2000\n",
    "EPOCHS = 50\n",
    "VERBOSE = 1\n",
    "ACTIVATION = 'relu'\n",
    "DROPOUT = 0.5\n",
    "GRU_UNIT = 64\n",
    "GRU2_UNIT = 16\n",
    "DENSE_UNIT = 32\n",
    "\n",
    "lstm_model_1 = [\n",
    "    GRU(GRU_UNIT,return_sequences=True,use_bias=True),\n",
    "    GRU(GRU2_UNIT,activation=ACTIVATION),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(DENSE_UNIT,activation=ACTIVATION),\n",
    "    Dense(1)\n",
    "]\n",
    "\n",
    "tags = {\"type.base\": \"RNN\",\n",
    "        \"type.sub\": \"GRU\",\n",
    "        \"domain\": \"exercise_score\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-insertion",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 100, 64)           17664     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 16)                3936      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 22,177\n",
      "Trainable params: 22,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 101s 48ms/step - loss: 0.9797 - mse: 0.9797 - mae: 0.7842 - rmse: 0.9896 - val_loss: 1.4312 - val_mse: 1.4312 - val_mae: 1.0647 - val_rmse: 1.1963\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 91s 46ms/step - loss: 0.8738 - mse: 0.8738 - mae: 0.7480 - rmse: 0.9348 - val_loss: 1.0535 - val_mse: 1.0535 - val_mae: 0.8309 - val_rmse: 1.0264\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 89s 45ms/step - loss: 0.8141 - mse: 0.8141 - mae: 0.7128 - rmse: 0.9021 - val_loss: 0.8708 - val_mse: 0.8708 - val_mae: 0.7831 - val_rmse: 0.9332\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 88s 44ms/step - loss: 0.7957 - mse: 0.7957 - mae: 0.7040 - rmse: 0.8920 - val_loss: 1.4224 - val_mse: 1.4224 - val_mae: 1.0393 - val_rmse: 1.1926\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 98s 49ms/step - loss: 0.7946 - mse: 0.7946 - mae: 0.7105 - rmse: 0.8913 - val_loss: 0.8864 - val_mse: 0.8864 - val_mae: 0.8189 - val_rmse: 0.9415\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 85s 42ms/step - loss: 0.8206 - mse: 0.8206 - mae: 0.7234 - rmse: 0.9057 - val_loss: 0.8233 - val_mse: 0.8233 - val_mae: 0.7780 - val_rmse: 0.9074\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 82s 41ms/step - loss: 0.7720 - mse: 0.7720 - mae: 0.6980 - rmse: 0.8783 - val_loss: 0.8962 - val_mse: 0.8962 - val_mae: 0.8317 - val_rmse: 0.9467\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 84s 42ms/step - loss: 0.6166 - mse: 0.6166 - mae: 0.6140 - rmse: 0.7851 - val_loss: 0.9070 - val_mse: 0.9070 - val_mae: 0.8171 - val_rmse: 0.9523\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 82s 41ms/step - loss: 0.7559 - mse: 0.7559 - mae: 0.6678 - rmse: 0.8694 - val_loss: 0.9187 - val_mse: 0.9187 - val_mae: 0.8489 - val_rmse: 0.9585\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 81s 41ms/step - loss: 0.7007 - mse: 0.7007 - mae: 0.6506 - rmse: 0.8369 - val_loss: 0.8441 - val_mse: 0.8441 - val_mae: 0.7996 - val_rmse: 0.9188\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 84s 42ms/step - loss: 0.6324 - mse: 0.6324 - mae: 0.6267 - rmse: 0.7951 - val_loss: 0.9100 - val_mse: 0.9100 - val_mae: 0.8209 - val_rmse: 0.9539\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 82s 41ms/step - loss: 0.5802 - mse: 0.5802 - mae: 0.5901 - rmse: 0.7617 - val_loss: 1.1118 - val_mse: 1.1118 - val_mae: 0.8813 - val_rmse: 1.0544\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 81s 41ms/step - loss: 0.5486 - mse: 0.5486 - mae: 0.5698 - rmse: 0.7406 - val_loss: 1.1422 - val_mse: 1.1422 - val_mae: 0.9147 - val_rmse: 1.0687\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 118s 59ms/step - loss: 0.5927 - mse: 0.5927 - mae: 0.6001 - rmse: 0.7697 - val_loss: 0.9620 - val_mse: 0.9620 - val_mae: 0.8564 - val_rmse: 0.9808\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 143s 71ms/step - loss: 0.5017 - mse: 0.5017 - mae: 0.5490 - rmse: 0.7083 - val_loss: 0.9940 - val_mse: 0.9940 - val_mae: 0.8372 - val_rmse: 0.9970\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 110s 55ms/step - loss: 0.6090 - mse: 0.6090 - mae: 0.6123 - rmse: 0.7801 - val_loss: 0.9756 - val_mse: 0.9756 - val_mae: 0.8531 - val_rmse: 0.9877\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 792s 396ms/step - loss: 0.5568 - mse: 0.5568 - mae: 0.5744 - rmse: 0.7462 - val_loss: 0.9847 - val_mse: 0.9847 - val_mae: 0.8545 - val_rmse: 0.9923\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 82s 41ms/step - loss: 0.5274 - mse: 0.5274 - mae: 0.5536 - rmse: 0.7262 - val_loss: 0.9706 - val_mse: 0.9706 - val_mae: 0.8355 - val_rmse: 0.9852\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 85s 42ms/step - loss: 0.5008 - mse: 0.5008 - mae: 0.5405 - rmse: 0.7075 - val_loss: 0.8502 - val_mse: 0.8502 - val_mae: 0.7787 - val_rmse: 0.9221\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 82s 41ms/step - loss: 0.5060 - mse: 0.5060 - mae: 0.5466 - rmse: 0.7112 - val_loss: 0.9086 - val_mse: 0.9086 - val_mae: 0.8278 - val_rmse: 0.9532\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 82s 41ms/step - loss: 0.5044 - mse: 0.5044 - mae: 0.5405 - rmse: 0.7101 - val_loss: 1.2496 - val_mse: 1.2496 - val_mae: 1.0157 - val_rmse: 1.1178\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 85s 42ms/step - loss: 0.4208 - mse: 0.4208 - mae: 0.4846 - rmse: 0.6486 - val_loss: 0.7274 - val_mse: 0.7274 - val_mae: 0.7284 - val_rmse: 0.8529\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 83s 41ms/step - loss: 0.4046 - mse: 0.4046 - mae: 0.4695 - rmse: 0.6360 - val_loss: 0.7355 - val_mse: 0.7355 - val_mae: 0.7332 - val_rmse: 0.8576\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 88s 44ms/step - loss: 0.4164 - mse: 0.4164 - mae: 0.4824 - rmse: 0.6448 - val_loss: 0.7170 - val_mse: 0.7170 - val_mae: 0.7341 - val_rmse: 0.8468\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 150s 75ms/step - loss: 0.4625 - mse: 0.4625 - mae: 0.5092 - rmse: 0.6795 - val_loss: 0.9817 - val_mse: 0.9817 - val_mae: 0.8503 - val_rmse: 0.9908\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 139s 69ms/step - loss: 0.6387 - mse: 0.6387 - mae: 0.6154 - rmse: 0.7980 - val_loss: 1.1494 - val_mse: 1.1494 - val_mae: 0.9235 - val_rmse: 1.0721\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 107s 54ms/step - loss: 0.6040 - mse: 0.6040 - mae: 0.6048 - rmse: 0.7772 - val_loss: 0.8858 - val_mse: 0.8858 - val_mae: 0.7854 - val_rmse: 0.9412\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 460s 230ms/step - loss: 0.5594 - mse: 0.5594 - mae: 0.5704 - rmse: 0.7478 - val_loss: 0.8429 - val_mse: 0.8429 - val_mae: 0.7337 - val_rmse: 0.9181\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 379s 189ms/step - loss: 0.6329 - mse: 0.6329 - mae: 0.6256 - rmse: 0.7950 - val_loss: 1.0979 - val_mse: 1.0979 - val_mae: 0.8738 - val_rmse: 1.0478\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 236s 118ms/step - loss: 0.5903 - mse: 0.5903 - mae: 0.5981 - rmse: 0.7681 - val_loss: 1.3183 - val_mse: 1.3183 - val_mae: 0.9372 - val_rmse: 1.1482\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 246s 123ms/step - loss: 0.6803 - mse: 0.6803 - mae: 0.6485 - rmse: 0.8243 - val_loss: 0.9475 - val_mse: 0.9475 - val_mae: 0.8378 - val_rmse: 0.9734\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 239s 120ms/step - loss: 0.5385 - mse: 0.5385 - mae: 0.5619 - rmse: 0.7338 - val_loss: 1.0138 - val_mse: 1.0138 - val_mae: 0.8613 - val_rmse: 1.0069\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 306s 153ms/step - loss: 0.5365 - mse: 0.5365 - mae: 0.5592 - rmse: 0.7323 - val_loss: 0.7185 - val_mse: 0.7185 - val_mae: 0.7706 - val_rmse: 0.8476\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 253s 127ms/step - loss: 0.5807 - mse: 0.5807 - mae: 0.5939 - rmse: 0.7619 - val_loss: 0.8617 - val_mse: 0.8617 - val_mae: 0.8153 - val_rmse: 0.9283\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 239s 120ms/step - loss: 0.5030 - mse: 0.5030 - mae: 0.5383 - rmse: 0.7091 - val_loss: 0.7859 - val_mse: 0.7859 - val_mae: 0.7841 - val_rmse: 0.8865\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 238s 119ms/step - loss: 0.4619 - mse: 0.4619 - mae: 0.5107 - rmse: 0.6796 - val_loss: 0.8798 - val_mse: 0.8798 - val_mae: 0.8117 - val_rmse: 0.9380\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 245s 122ms/step - loss: 0.4219 - mse: 0.4219 - mae: 0.4825 - rmse: 0.6495 - val_loss: 0.8699 - val_mse: 0.8699 - val_mae: 0.8066 - val_rmse: 0.9327\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 316s 158ms/step - loss: 0.4165 - mse: 0.4165 - mae: 0.4845 - rmse: 0.6449 - val_loss: 0.8063 - val_mse: 0.8063 - val_mae: 0.8152 - val_rmse: 0.8979\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 237s 119ms/step - loss: 0.4127 - mse: 0.4127 - mae: 0.4804 - rmse: 0.6423 - val_loss: 0.7900 - val_mse: 0.7900 - val_mae: 0.7841 - val_rmse: 0.8888\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 241s 120ms/step - loss: 0.4091 - mse: 0.4091 - mae: 0.4758 - rmse: 0.6396 - val_loss: 0.8815 - val_mse: 0.8815 - val_mae: 0.8458 - val_rmse: 0.9389\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 2856s 1s/step - loss: 0.4636 - mse: 0.4636 - mae: 0.5083 - rmse: 0.6806 - val_loss: 0.8629 - val_mse: 0.8629 - val_mae: 0.8196 - val_rmse: 0.9289\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 95s 48ms/step - loss: 0.4985 - mse: 0.4985 - mae: 0.5231 - rmse: 0.7055 - val_loss: 0.6399 - val_mse: 0.6399 - val_mae: 0.7047 - val_rmse: 0.7999\n",
      "Epoch 43/50\n",
      " 712/2000 [=========>....................] - ETA: 1:00 - loss: 0.4641 - mse: 0.4641 - mae: 0.5084 - rmse: 0.6802"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "with mlflow.start_run(run_name=model_name) as run:\n",
    "    mlflow.set_tags(tags)\n",
    "\n",
    "    model = create_model(layers=lstm_model_1, optimizer=OPTIMIZER)\n",
    "    \n",
    "    history = model.fit(train_set, \n",
    "                        epochs=EPOCHS, \n",
    "                        steps_per_epoch=EVALUATION_INTERVAL, \n",
    "                        validation_data=test_set, \n",
    "                        validation_steps=50,\n",
    "                        verbose=VERBOSE, \n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_train_history(history, 'Training and validation loss')\n",
    "    plt.savefig(\"training_history.jpg\")\n",
    "    mlflow.log_artifact(\"training_history.jpg\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # Log model parameters\n",
    "    mlflow.log_param(\"activation\", ACTIVATION)\n",
    "    mlflow.log_param(\"optimizer\", OPTIMIZER)\n",
    "    mlflow.log_param(\"evaluation interval\", EVALUATION_INTERVAL)\n",
    "    mlflow.log_param(\"epochs\", early_stopping.stopped_epoch)\n",
    "    mlflow.log_param(\"total params\", model.count_params())\n",
    "    mlflow.log_param(\"C\", HISTORY_SIZE)\n",
    "    mlflow.log_param(\"Dropout\", DROPOUT)\n",
    "    mlflow.log_param(\"GRU units\", GRU_UNIT)\n",
    "    mlflow.log_param(\"GRU 2 units\", GRU2_UNIT)\n",
    "    mlflow.log_param(\"Dense units\", DENSE_UNIT)\n",
    "    mlflow.log_param(\"step size\", STEP)\n",
    "    mlflow.log_param(\"input mean\", train_mean)\n",
    "    mlflow.log_param(\"input std\", train_std)\n",
    "    mlflow.log_param(\"output mean\", target_mean)\n",
    "    mlflow.log_param(\"output std\", target_std)\n",
    "\n",
    "    # Log model prediction time\n",
    "    predictions = None\n",
    "    times = []\n",
    "    for i in range(10):\n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(X_test,verbose=1)\n",
    "        end_time = time.time()\n",
    "        process_time = (end_time - start_time) * 1000\n",
    "        times.append(process_time)\n",
    "    process_time = sum(times) / len(times) / len(predictions)\n",
    "\n",
    "    # Log model performance\n",
    "    predictions = predictions * target_std + target_mean\n",
    "    actual = y_test * target_std + target_mean\n",
    "\n",
    "    (mse, mae, r2, variance) = eval_metrics(actual, predictions)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"R-squared\", r2)\n",
    "    mlflow.log_metric(\"variance\", variance)\n",
    "    mlflow.log_metric(\"process time\", process_time)\n",
    "    \n",
    "    # Plot test prediction\n",
    "    w = 10\n",
    "    h = 8\n",
    "    d = 70\n",
    "    plt.figure(figsize=(w, h), dpi=d)\n",
    "    a = model.predict(X_test,verbose=1).reshape(-1) * target_std + target_mean\n",
    "    b = y_test* target_std + target_mean\n",
    "    y1=y_test.reshape(len(predictions))\n",
    "    y2=predictions.reshape(len(predictions))\n",
    "    x = np.arange(len(y_test))\n",
    "    seaborn.scatterplot(x, a)\n",
    "    seaborn.scatterplot(x, b)\n",
    "    plt.savefig(\"sample_predictions.jpg\")\n",
    "    mlflow.log_artifact(\"sample_predictions.jpg\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Log model and scaler(s)\n",
    "    mlflow.keras.log_model(model, model_name, signature=signature)\n",
    "#     mlflow.sklearn.log_model(X_scaler, 'InputScaler')\n",
    "#     mlflow.sklearn.log_model(y_scaler, 'OutputScaler')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-poultry",
   "metadata": {},
   "source": [
    "## 3.2 Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_history(history, 'Training and validation loss')\n",
    "#plt.savefig(\"training_history.jpg\")\n",
    "#mlflow.log_artifact(\"training_history.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-flight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
