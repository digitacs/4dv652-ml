{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - Problem B (PoseNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# General requirements\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# MLflow dashboard\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://35.228.45.76:5000')\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='../../keys/mlflow-312506-8cfad529f4fd.json'\n",
    "\n",
    "# Import data augmentation\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from augmentation.methods import *\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Set random state\n",
    "import numpy as np\n",
    "random_state = 47\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# Use GPU (if available)\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices( 'GPU' )\n",
    "print( 'Num GPUs Available: ', len( physical_devices ) )\n",
    "if len( physical_devices ) > 0:  \n",
    "    tf.config.experimental.set_memory_growth( physical_devices[0], True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n",
    "\n",
    "## 1.1 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posenet_dataset_path = '../../datasets/posenet-uncut/'\n",
    "kinect_dataset_path = '../../datasets/kinect_fixed_not_cut/'\n",
    "cut_dataset_path = '../../datasets/new_posenet_marked_start_end/'\n",
    "goodness_score = pd.read_csv('../../datasets/VideoScoring.csv')\n",
    "exrecise_score = pd.read_csv('../../datasets/ExerciseScoring.csv')\n",
    "\n",
    "train_test_ratio = 0.9\n",
    "new_label = 'ExreciseScore'\n",
    "exersice_score_indicator = 'O_Score'\n",
    "goodness_score_indicator = 'AVG'\n",
    "goodness_score_threshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop video augmentation results\n",
    "goodness_score = goodness_score[goodness_score['FileName'].str.match('U')==False]\n",
    "goodness_score = goodness_score[goodness_score[goodness_score_indicator]>=goodness_score_threshold]\n",
    "\n",
    "posenet_ok = goodness_score['FileName']+'.csv'\n",
    "\n",
    "exrecise_score['Posenet'] = exrecise_score['Posenet'] + '.csv'\n",
    "exrecise_score['Kinect'] = exrecise_score['Kinect'] + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posenet datasets: 157\n",
      "Total kinect datasets: 157\n"
     ]
    }
   ],
   "source": [
    "posenet_files = []\n",
    "kinect_files = []\n",
    "\n",
    "for file in os.listdir(posenet_dataset_path):\n",
    "    if not file.find(\".csv\",0) == -1:\n",
    "        if file in (goodness_score['FileName']+'.csv').to_list():\n",
    "            posenet_files.append(file)\n",
    "        \n",
    "for file in os.listdir(kinect_dataset_path):\n",
    "    if not file.find(\".csv\",0) == -1:\n",
    "        if file in (goodness_score['FileName']+'_kinect.csv').to_list():\n",
    "            kinect_files.append(file)\n",
    "        \n",
    "print('Total posenet datasets: {}'.format(len(posenet_files)))\n",
    "print('Total kinect datasets: {}'.format(len(kinect_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 141\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SPLIT_POSENET = int(len(posenet_files)*train_test_ratio)\n",
    "TRAIN_SPLIT_KINECT = int(len(kinect_files)*train_test_ratio)\n",
    "print(TRAIN_SPLIT_POSENET,TRAIN_SPLIT_KINECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(df):\n",
    "    augmented_datasets = []\n",
    "    \n",
    "    # Add original data\n",
    "    augmented_datasets.append(df)\n",
    "    \n",
    "    # Mirror X coordinate\n",
    "    for i in range(len(augmented_datasets)):\n",
    "        augmented_datasets.append(mirror(augmented_datasets[i],'x', append=False))\n",
    "\n",
    "    # Stretch by 50%\n",
    "    for i in range(len(augmented_datasets)):\n",
    "        df_temp = augMultiplier(augmented_datasets[i].drop(columns=[new_label]), multiplier=1.5)\n",
    "        df_temp[new_label] = augmented_datasets[i][new_label]\n",
    "        augmented_datasets.append(df_temp)\n",
    "    \n",
    "\n",
    "    # Compress by 25%\n",
    "    for i in range(len(augmented_datasets)):\n",
    "        df_temp = augMultiplier(augmented_datasets[i].drop(columns=[new_label]), multiplier=0.25)\n",
    "        df_temp[new_label] = augmented_datasets[i][new_label]\n",
    "        augmented_datasets.append(df_temp)\n",
    "   \n",
    "\n",
    "    # Rotate by p/7\n",
    "#     samples = df.sample(5000)\n",
    "#     angle = 3.1415 / 7\n",
    "#     samples_rotated = rotate(samples.drop(columns=[new_label]), angle=angle, posenet=False)\n",
    "#     samples_rotated[new_label] = samples[new_label].append(samples[new_label], ignore_index=True)\n",
    "#     df = df.append(samples_rotated, ignore_index=True)\n",
    "#     print(df.shape)\n",
    "\n",
    "    # Rotate by -p/9\n",
    "#     samples = df.sample(5000)\n",
    "#     angle = 3.1415 / -9\n",
    "#     samples_rotated = rotate(samples.drop(columns=[new_label]), angle=angle, posenet=False)\n",
    "#     samples_rotated[new_label] = samples[new_label].append(samples[new_label], ignore_index=True)\n",
    "#     df = df.append(samples_rotated, ignore_index=True)\n",
    "#     print(df.shape)\n",
    "\n",
    "    return augmented_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY_SIZE = 10\n",
    "STEP = 20\n",
    "N_FEATURES = 26\n",
    "\n",
    "def convert_data(dataset, target_column, history_columns):\n",
    "\n",
    "    segments = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(0,len(dataset) - HISTORY_SIZE, STEP):\n",
    "        segments.append(dataset[history_columns].values[i:i + HISTORY_SIZE])\n",
    "        labels.append(dataset[target_column].mean())\n",
    "    \n",
    "    return np.array(segments).reshape(-1,HISTORY_SIZE*N_FEATURES), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_columns = ['head_x', 'head_y', 'left_shoulder_x', 'left_shoulder_y',\n",
    "       'right_shoulder_x', 'right_shoulder_y', 'left_elbow_x', 'left_elbow_y',\n",
    "       'right_elbow_x', 'right_elbow_y', 'left_wrist_x', 'left_wrist_y',\n",
    "       'right_wrist_x', 'right_wrist_y', 'left_hip_x', 'left_hip_y',\n",
    "       'right_hip_x', 'right_hip_y', 'left_knee_x', 'left_knee_y',\n",
    "       'right_knee_x', 'right_knee_y', 'left_ankle_x', 'left_ankle_y',\n",
    "       'right_ankle_x', 'right_ankle_y']\n",
    "target_column = 'ExreciseScore'\n",
    "\n",
    "def read_dataset(path,file_list,split_point,is_posenet=True,isTrain=True):\n",
    "    all_segments = None\n",
    "    all_labels = None\n",
    "    \n",
    "    start=0\n",
    "    end=0\n",
    "    \n",
    "    if isTrain:\n",
    "        start = 0\n",
    "        end = split_point\n",
    "    else:\n",
    "        start = split_point\n",
    "        end = len(file_list)\n",
    "    \n",
    "    for file in file_list[start:end]:\n",
    "        df = pd.read_csv(path+file)\n",
    "        \n",
    "        if is_posenet:  \n",
    "            df = df[df.columns.drop(list(df.filter(regex='_eye_')))]\n",
    "            df = df[df.columns.drop(list(df.filter(regex='_ear_')))]\n",
    "            df = df[df.columns.drop(list(df.filter(regex='score')))]\n",
    "            df = df.rename(columns={'nose_x': 'head_x', 'nose_y': 'head_y'})\n",
    "            df[new_label] = float(exrecise_score[exrecise_score['Posenet'] == file][exersice_score_indicator])\n",
    "        else:  \n",
    "            df = df.drop(columns=['Unnamed: 0','FrameNo'])\n",
    "            df[new_label] = float(exrecise_score[exrecise_score['Kinect'] == file][exersice_score_indicator])\n",
    "            \n",
    "        # Cut start / end\n",
    "        try:\n",
    "            se = pd.read_csv('../../datasets/new_posenet_marked_start_end/'+file)\n",
    "            cut_start = min(se[(se['start']==0) * (se['end']==0)].index)\n",
    "            cut_end = max(se[(se['start']==0) * (se['end']==0)].index)\n",
    "            df = df.iloc[cut_start:cut_end]\n",
    "        except IOError as e:\n",
    "            print('Error in reading file: ', e)\n",
    "         \n",
    "        if all_segments is None:\n",
    "            all_segments = df[history_columns]\n",
    "            all_labels = df[target_column]\n",
    "        else:\n",
    "            all_segments = np.append(all_segments,df[history_columns],axis=0)  \n",
    "            all_labels = np.append(all_labels,df[target_column],axis=0)\n",
    "            \n",
    "    \n",
    "        if isTrain:\n",
    "            for sets in data_augmentation(df):\n",
    "                all_segments = np.append(all_segments,sets[history_columns],axis=0)  \n",
    "                all_labels = np.append(all_labels,sets[target_column],axis=0)  \n",
    "\n",
    "                sys.stdout.write(\"Total added rows: %d   \\r\" % (all_segments.shape[0]) )\n",
    "                sys.stdout.flush()\n",
    "        \n",
    "    \n",
    "    return all_segments,all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in reading file:  [Errno 2] No such file or directory: '../../datasets/new_posenet_marked_start_end/A60.csv'\n",
      "Total added rows: 640053   \r"
     ]
    }
   ],
   "source": [
    "X_train, y_train = read_dataset(posenet_dataset_path,posenet_files,TRAIN_SPLIT_POSENET,True,True)\n",
    "X_test, y_test = read_dataset(posenet_dataset_path,posenet_files,TRAIN_SPLIT_POSENET,True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640053, 26)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6689, 26)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.      , 0.      , 0.      , ..., 3.948862, 3.948862, 3.948862])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Standardize features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (640053, 26)\n",
      "Training labels shape: (640053, 1) \n",
      "\n",
      "Test features shape: (6689, 26)\n",
      "Test labels shape: (6689,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train = y_scaler.fit_transform(y_train.reshape(-1,1))\n",
    "\n",
    "print('Training features shape:', X_train.shape)\n",
    "print('Training labels shape:', y_train.shape, '\\n')\n",
    "\n",
    "print('Test features shape:', X_test.shape)\n",
    "print('Test labels shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Reshape data for convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: [[ 0.87950181 -0.44858478  0.87045823 ...  0.14037643  0.88156627\n",
      "   0.14203014]\n",
      " [ 0.87950181 -0.44858478  0.87045823 ...  0.14037643  0.88156627\n",
      "   0.14203014]\n",
      " [ 0.88387521 -0.44255569  0.8770654  ...  0.137868    0.88232331\n",
      "   0.14709143]\n",
      " ...\n",
      " [-0.5234906  -0.92524202 -0.52601899 ... -0.95120237 -0.52541417\n",
      "  -0.94454015]\n",
      " [-0.52298005 -0.92331228 -0.52671667 ... -0.95192255 -0.52542601\n",
      "  -0.94421066]\n",
      " [-0.52298005 -0.92331228 -0.52671667 ... -0.95192255 -0.52542601\n",
      "  -0.94421066]]\n",
      "After: (640053, 1, 26)\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "\n",
    "original_shape = X_train\n",
    "X_train = X_train.reshape((original_shape.shape[0], C, original_shape.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], C, X_test.shape[1]))\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "\n",
    "print(\"Before: {}\".format(original_shape))\n",
    "\n",
    "input_shape = (n_timesteps, n_features)\n",
    "\n",
    "print(\"After: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Model Architecture\n",
    "\n",
    "## 2.1 Model Architecture\n",
    "\n",
    "The **create_model()** method returns a model with the layers and configurations specified.\n",
    "\n",
    "The model will optimize the mean squared error (mse) required for regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError, RootMeanSquaredError\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "\n",
    "metrics = [\n",
    "    MeanSquaredError(name=\"mse\", dtype=None),\n",
    "    MeanAbsoluteError(name=\"mae\", dtype=None),\n",
    "    RootMeanSquaredError(name=\"rmse\", dtype=None),\n",
    "]\n",
    "\n",
    "def create_model(layers, optimizer):\n",
    "    model = tf.keras.models.Sequential(\n",
    "        InputLayer(input_shape=(n_timesteps, n_features))\n",
    "    )\n",
    "\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=metrics)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run Experiments\n",
    "\n",
    "We will use our [MLflow dashboard](http://35.228.45.76:5000/#/) to track the outcome of experimentation runs.\n",
    "\n",
    "## 3.1 Define a MLflow Experiment\n",
    "\n",
    "We'll define the running setup and the model signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verbose, epochs, batch_size = 1, 150, 256\n",
    "#verbose, epochs, batch_size = 1, 150, 128\n",
    "#verbose, epochs, batch_size = 1, 150, 64\n",
    "verbose, epochs, batch_size = 1, 150, 32\n",
    "#verbose, epochs, batch_size = 1, 150, 16\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='min',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "input_schema = Schema([\n",
    "    ColSpec(\"double\", \"head_x\"),\n",
    "    ColSpec(\"double\", \"head_y\"),\n",
    "    ColSpec(\"double\", \"left_shoulder_x\"),\n",
    "    ColSpec(\"double\", \"left_shoulder_y\"),\n",
    "    ColSpec(\"double\", \"right_shoulder_x\"),\n",
    "    ColSpec(\"double\", \"right_shoulder_y\"),\n",
    "    ColSpec(\"double\", \"left_elbow_x\"),\n",
    "    ColSpec(\"double\", \"left_elbow_y\"),\n",
    "    ColSpec(\"double\", \"right_elbow_x\"),\n",
    "    ColSpec(\"double\", \"right_elbow_y\"),\n",
    "    ColSpec(\"double\", \"left_wrist_x\"),\n",
    "    ColSpec(\"double\", \"left_wrist_y\"),\n",
    "    ColSpec(\"double\", \"right_wrist_x\"),\n",
    "    ColSpec(\"double\", \"right_wrist_y\"),\n",
    "    ColSpec(\"double\", \"left_hip_x\"),\n",
    "    ColSpec(\"double\", \"left_hip_y\"),\n",
    "    ColSpec(\"double\", \"right_hip_x\"),\n",
    "    ColSpec(\"double\", \"right_hip_y\"),\n",
    "    ColSpec(\"double\", \"left_knee_x\"),\n",
    "    ColSpec(\"double\", \"left_knee_y\"),\n",
    "    ColSpec(\"double\", \"right_knee_x\"),\n",
    "    ColSpec(\"double\", \"right_knee_y\"),\n",
    "    ColSpec(\"double\", \"left_ankle_x\"),\n",
    "    ColSpec(\"double\", \"left_ankle_y\"),\n",
    "    ColSpec(\"double\", \"right_ankle_x\"),\n",
    "    ColSpec(\"double\", \"right_ankle_y\"),\n",
    "])\n",
    "output_schema = Schema([\n",
    "    ColSpec(\"double\", \"ExreciseScore\")\n",
    "])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Plot Training History\n",
    "\n",
    "This method will be used to inspect how the model is learning by showing training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "def plot_train_history(history, title):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Define Evaluation Metrics\n",
    "\n",
    "We'll use standard regression performance metrics to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    mse = mean_squared_error(actual, pred)\n",
    "    msa = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    variance = explained_variance_score(actual, pred)\n",
    "    return mse, msa, r2, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Model Parameters\n",
    "\n",
    "For each experimentation run, we'll test different values for model parameters with each model architecture we define. These are the parameter values we're using as a starting point:\n",
    "\n",
    "- We'll begin small kernels, and then increase to larger ones. \n",
    "- As for layers, we'll start by using the ReLU activation function and He weight initialization, which we have noticed seem to provide a general good model performance.\n",
    "- The RMSprop optimizer will be used first as this is a regression problem we're trying to solve, and we'll use a modest learning rate of 0.001 with a large momentum of 0.9 (generally good practices). But we'll also try the Adam optimizer.\n",
    "\n",
    "As we're experimenting with these values, the results will be shown in [MLflow dashboard](http://35.228.45.76:5000/#/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "#filters = 16\n",
    "filters = 32\n",
    "\n",
    "kernel_size = 3\n",
    "#kernel_size = 5\n",
    "\n",
    "activation = 'relu'\n",
    "kernel_initializer = 'he_uniform'\n",
    "\n",
    "pool_size = 2\n",
    "\n",
    "dense_units = 32\n",
    "#dense_units = 64\n",
    "\n",
    "output_activation = ''\n",
    "#output_activation = 'linear'\n",
    "\n",
    "# Optimization function\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.01\n",
    "#opt = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "opt = Adam(learning_rate=learning_rate)\n",
    "#opt = RMSprop(learning_rate=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Model\n",
    "\n",
    "Now we can start experimenting with the models, and we'll start with a baseline model. The baseline model will establish a minimum model performance to which all other models can be compared. \n",
    "\n",
    "For the baseline model, we've been inspired by the general architectural principles of VGG models - but with 1 dimensional convolution. \n",
    "This seems like a good starting point because that structure is easy to understand and implement at the same time as it has previously achieved good performance. <br />\n",
    "The architecture involves stacking convolutional layers with small 3x3 filters followed by a max-pooling layer. Padding is used on the convolutional layers to ensure the height and width of the output feature maps matches the inputs.\n",
    "\n",
    "The feature maps output from the feature extraction part of this baseline model will be flattened before we can interpret them with two fully connected (Dense) layers, and then output a prediction.\n",
    "\n",
    "This baseline will also be tested with larger filters as we're experimenting with parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 1, 32)             2528      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1, 32)             3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 6,721\n",
      "Trainable params: 6,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "16002/16002 [==============================] - 19s 1ms/step - loss: 0.7631 - mse: 0.7631 - mae: 0.6950 - rmse: 0.8722 - val_loss: 0.9063 - val_mse: 0.9063 - val_mae: 0.7052 - val_rmse: 0.9520\n",
      "Epoch 2/150\n",
      "16002/16002 [==============================] - 23s 1ms/step - loss: 0.5619 - mse: 0.5619 - mae: 0.5881 - rmse: 0.7496 - val_loss: 0.7544 - val_mse: 0.7544 - val_mae: 0.6340 - val_rmse: 0.8686\n",
      "Epoch 3/150\n",
      "16002/16002 [==============================] - 25s 2ms/step - loss: 0.5275 - mse: 0.5275 - mae: 0.5650 - rmse: 0.7263 - val_loss: 0.7096 - val_mse: 0.7096 - val_mae: 0.6052 - val_rmse: 0.8424\n",
      "Epoch 4/150\n",
      "16002/16002 [==============================] - 21s 1ms/step - loss: 0.5033 - mse: 0.5033 - mae: 0.5464 - rmse: 0.7094 - val_loss: 0.6918 - val_mse: 0.6918 - val_mae: 0.6084 - val_rmse: 0.8318\n",
      "Epoch 5/150\n",
      "16002/16002 [==============================] - 22s 1ms/step - loss: 0.4811 - mse: 0.4811 - mae: 0.5316 - rmse: 0.6936 - val_loss: 0.7878 - val_mse: 0.7878 - val_mae: 0.6413 - val_rmse: 0.8876\n",
      "Epoch 6/150\n",
      "16002/16002 [==============================] - 22s 1ms/step - loss: 0.4801 - mse: 0.4801 - mae: 0.5294 - rmse: 0.6928 - val_loss: 0.7231 - val_mse: 0.7231 - val_mae: 0.6117 - val_rmse: 0.8504\n",
      "Epoch 7/150\n",
      "16002/16002 [==============================] - 23s 1ms/step - loss: 0.4616 - mse: 0.4616 - mae: 0.5210 - rmse: 0.6794 - val_loss: 0.7030 - val_mse: 0.7030 - val_mae: 0.5994 - val_rmse: 0.8385\n",
      "Epoch 8/150\n",
      "16002/16002 [==============================] - 23s 1ms/step - loss: 0.4609 - mse: 0.4609 - mae: 0.5177 - rmse: 0.6789 - val_loss: 0.6214 - val_mse: 0.6214 - val_mae: 0.5486 - val_rmse: 0.7883\n",
      "Epoch 9/150\n",
      "16002/16002 [==============================] - 25s 2ms/step - loss: 0.4439 - mse: 0.4439 - mae: 0.5087 - rmse: 0.6662 - val_loss: 0.6457 - val_mse: 0.6457 - val_mae: 0.5760 - val_rmse: 0.8035\n",
      "Epoch 10/150\n",
      "16002/16002 [==============================] - 24s 1ms/step - loss: 0.4305 - mse: 0.4305 - mae: 0.5012 - rmse: 0.6561 - val_loss: 0.8170 - val_mse: 0.8170 - val_mae: 0.6842 - val_rmse: 0.9039\n",
      "Epoch 11/150\n",
      "16002/16002 [==============================] - 26s 2ms/step - loss: 0.4182 - mse: 0.4182 - mae: 0.4937 - rmse: 0.6466 - val_loss: 0.7833 - val_mse: 0.7833 - val_mae: 0.6106 - val_rmse: 0.88504940 - rmse: 0. - ETA: 1s - loss: 0.4186 - mse: 0 - ETA: 0s - loss: 0.4183 - mse: 0.4183 - mae: 0.4938 - rm\n",
      "Epoch 12/150\n",
      "16002/16002 [==============================] - 24s 2ms/step - loss: 0.4042 - mse: 0.4042 - mae: 0.4835 - rmse: 0.6357 - val_loss: 0.7228 - val_mse: 0.7228 - val_mae: 0.6083 - val_rmse: 0.8502\n",
      "Epoch 13/150\n",
      "16002/16002 [==============================] - 25s 2ms/step - loss: 0.3896 - mse: 0.3896 - mae: 0.4741 - rmse: 0.6241 - val_loss: 0.6461 - val_mse: 0.6461 - val_mae: 0.5504 - val_rmse: 0.8038\n",
      "Epoch 14/150\n",
      "16002/16002 [==============================] - 20s 1ms/step - loss: 0.3825 - mse: 0.3825 - mae: 0.4690 - rmse: 0.6184 - val_loss: 0.7710 - val_mse: 0.7710 - val_mae: 0.5956 - val_rmse: 0.8781\n",
      "Epoch 15/150\n",
      "16002/16002 [==============================] - 22s 1ms/step - loss: 0.3804 - mse: 0.3804 - mae: 0.4673 - rmse: 0.6167 - val_loss: 0.6579 - val_mse: 0.6579 - val_mae: 0.5609 - val_rmse: 0.8111\n",
      "Epoch 16/150\n",
      "16002/16002 [==============================] - 23s 1ms/step - loss: 0.3694 - mse: 0.3694 - mae: 0.4600 - rmse: 0.6078 - val_loss: 0.7132 - val_mse: 0.7132 - val_mae: 0.6081 - val_rmse: 0.8445\n",
      "Epoch 17/150\n",
      "16002/16002 [==============================] - 22s 1ms/step - loss: 0.3775 - mse: 0.3775 - mae: 0.4651 - rmse: 0.6144 - val_loss: 0.9350 - val_mse: 0.9350 - val_mae: 0.7592 - val_rmse: 0.9670\n",
      "Epoch 18/150\n",
      " 1890/16002 [==>...........................] - ETA: 18s - loss: 0.4770 - mse: 0.4770 - mae: 0.5311 - rmse: 0.6884- ETA: 20s - loss: 0.6159 - "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tensorflow.keras.layers import Conv1D, Dropout, MaxPool1D, Flatten, Dense\n",
    "\n",
    "model_name = 'scoring_cnn_posenet'\n",
    "\n",
    "layers = [\n",
    "    Conv1D(filters=filters, kernel_size=kernel_size, padding='same', activation=activation, kernel_initializer=kernel_initializer),\n",
    "    MaxPool1D(pool_size=pool_size, padding='same'),\n",
    "    Conv1D(filters=filters, kernel_size=kernel_size, padding='same', activation=activation, kernel_initializer=kernel_initializer),\n",
    "    MaxPool1D(pool_size=pool_size, padding='same'),\n",
    "    #Dropout(0.2),\n",
    "    Flatten(), \n",
    "    Dense(dense_units, activation=activation, kernel_initializer=kernel_initializer),\n",
    "    Dense(n_outputs)\n",
    "    #Dense(n_outputs, activation=output_activation)\n",
    "]\n",
    "\n",
    "with mlflow.start_run(run_name=model_name) as run:\n",
    "\n",
    "    model = create_model(layers=layers, optimizer=opt)\n",
    "\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        validation_split=0.2, \n",
    "                        shuffle=True, \n",
    "                        verbose=verbose, \n",
    "                        callbacks=[early_stopping])\n",
    "\n",
    "    # Plot training history\n",
    "    plot_train_history(history, 'Training and validation loss')\n",
    "    plt.savefig(\"training_history.jpg\")\n",
    "    mlflow.log_artifact(\"training_history.jpg\")\n",
    "    plt.show()\n",
    "\n",
    "    # Log model parameters\n",
    "    mlflow.log_param(\"activation\", activation)\n",
    "    mlflow.log_param(\"kernel_initializer\", kernel_initializer)\n",
    "    mlflow.log_param(\"output activation\", output_activation)\n",
    "    mlflow.log_param(\"optimizer\", optimizer)\n",
    "    mlflow.log_param(\"learning rate\", learning_rate)\n",
    "    mlflow.log_param(\"batch size\", batch_size)\n",
    "    mlflow.log_param(\"epochs\", early_stopping.stopped_epoch)\n",
    "    mlflow.log_param(\"filters\", filters)\n",
    "    mlflow.log_param(\"kernel_size\", kernel_size)\n",
    "    mlflow.log_param(\"pool size\", pool_size)\n",
    "    mlflow.log_param(\"total params\", model.count_params())\n",
    "    mlflow.log_param(\"units\", dense_units)\n",
    "\n",
    "    # Log model prediction time\n",
    "    predictions = None\n",
    "    times = []\n",
    "    for i in range(10):\n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        process_time = (end_time - start_time) * 1000\n",
    "        times.append(process_time)\n",
    "    process_time = sum(times) / len(times)\n",
    "\n",
    "    # Log model performance\n",
    "    predictions = y_scaler.inverse_transform(predictions)\n",
    "    (mse, mae, r2, variance) = eval_metrics(y_test, predictions)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"R-squared\", r2)\n",
    "    mlflow.log_metric(\"variance\", variance)\n",
    "    mlflow.log_metric(\"process time\", process_time)\n",
    "\n",
    "    # Log model and scaler(s)\n",
    "    mlflow.keras.log_model(model, model_name, signature=signature)\n",
    "    mlflow.sklearn.log_model(X_scaler, 'InputScaler')\n",
    "    mlflow.sklearn.log_model(y_scaler, 'OutputScaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test, verbose=1)\n",
    "# Invert transform on predictions\n",
    "predictions = y_scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mse, msa, r2, variance) = eval_metrics(y_test, predictions)\n",
    "\n",
    "print('MSE: ', mse)\n",
    "print('MSA: ', msa)\n",
    "print('R-Squared: ', r2)\n",
    "print('Explained Variance Score: ', variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- No difference between optimizers when using large batch sizes (except SGD performed slightly worse). When using the largest batch size, the model always stopped training early (epochs = 100).\n",
    "\n",
    "- No activation function in output layer.\n",
    "\n",
    "- Small batch sizes did not give any good performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
