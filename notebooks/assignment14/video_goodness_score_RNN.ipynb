{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "rational-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "# MLflow dashboard\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://35.228.45.76:5000')\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='../../keys/mlflow-312506-8cfad529f4fd.json'\n",
    "\n",
    "# Import data augmentation\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from augmentation.methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "light-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "collaborative-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 47\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bacterial-precipitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices( 'GPU' )\n",
    "print( 'Num GPUs Available: ', len( physical_devices ) )\n",
    "if len( physical_devices ) > 0:\n",
    "    tf.config.experimental.set_memory_growth( physical_devices[0], True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-spider",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n",
    "## 1.1 Load the Posenet files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "steady-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dataset_path = '../../datasets/posenet-uncut/'\n",
    "ugly_dataset_path = '../../datasets/good_ugly_posenet/'\n",
    "subjective_score = pd.read_csv('../../datasets/VideoScoring.csv')\n",
    "\n",
    "all_data_X = []\n",
    "all_data_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "blessed-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_scores(file_name,ugly=False):\n",
    "    path = normal_dataset_path\n",
    "    prefix = ''\n",
    "    \n",
    "    if ugly:\n",
    "        path = ugly_dataset_path\n",
    "        prefix = 'U' \n",
    "        \n",
    "    df = pd.read_csv(path+file_name)\n",
    "    \n",
    "    df = df[df.columns.drop(list(df.filter(regex='_eye_')))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex='_ear_')))]\n",
    "    df = df.rename(columns={'nose_x': 'head_x', 'nose_y': 'head_y', 'nose_score': 'head_score'})\n",
    "        \n",
    "    df['ConfidenceScore'] = np.mean(df[list(df.filter(regex='_score'))].mean(axis=1) * df['score'])\n",
    "    sub_score = subjective_score.loc[subjective_score['FileName'] == prefix + file_name.replace('.csv', '')]['AVG']\n",
    "    df['GoodnessScore'] = float(sub_score)\n",
    "    \n",
    "    return df.drop(columns=['ConfidenceScore', 'GoodnessScore']), df[['ConfidenceScore', 'GoodnessScore']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "short-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scores to the original videos\n",
    "for file in os.listdir(normal_dataset_path):\n",
    "    if not file.find(\".csv\",0) == -1:\n",
    "        X,y = add_scores(file,False)\n",
    "        \n",
    "        all_data_X.append(X)\n",
    "        all_data_y.append(y)\n",
    "\n",
    "# Add scores to augmented videos\n",
    "for file in os.listdir(ugly_dataset_path):\n",
    "    if not file.find(\".csv\",0) == -1:\n",
    "        X,y = add_scores(file,True)\n",
    "        \n",
    "        all_data_X.append( X )\n",
    "        all_data_y.append( y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "progressive-philippines",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SPLIT = int(len(all_data_X)*0.9)\n",
    "TRAIN_SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "objective-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = all_data_X[:TRAIN_SPLIT]\n",
    "test_X = all_data_X[TRAIN_SPLIT:]\n",
    "train_y = all_data_y[:TRAIN_SPLIT]\n",
    "test_y = all_data_y[TRAIN_SPLIT:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "needed-hollywood",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "inputScaler = StandardScaler()\n",
    "\n",
    "for i in range(0,len(train_X)):\n",
    "    inputScaler.fit(train_X[i])\n",
    "for i in range(0,len(train_X)):\n",
    "    train_X[i] = inputScaler.transform(train_X[i])\n",
    "    \n",
    "for i in range(0,len(test_X)):\n",
    "    test_X[i] = inputScaler.transform(test_X[i])\n",
    "\n",
    "outputScaler = StandardScaler()\n",
    "for i in range(0,len(train_y)):\n",
    "    outputScaler.fit(train_y[i])\n",
    "for i in range(0,len(train_y)):\n",
    "    train_y[i] = outputScaler.transform(train_y[i])\n",
    "    \n",
    "for i in range(0,len(test_y)):\n",
    "    test_y[i] = outputScaler.transform(test_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "administrative-dairy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "(580, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_X[c].shape[1])\n",
    "print(train_y[c].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "handy-gross",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def multivariate_data(dataset, target, start_series, end_series, history_size,\n",
    "                      target_size, step):\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    l = 0\n",
    "\n",
    "    for c in range(start_series, end_series):\n",
    "        temp = np.array(dataset[c][:,0])\n",
    "        start_index = history_size\n",
    "        end_index = len(temp) - target_size\n",
    "        for i in range(start_index, end_index):\n",
    "            indices = range(i-history_size, i, step)\n",
    "            features = []\n",
    "\n",
    "            for l in range(0,dataset[c].shape[1]):\n",
    "                features.append(np.transpose(np.array(dataset[c][:,l])[indices]))\n",
    "\n",
    "        data.append(np.transpose(np.array(features)))\n",
    "        labels.append(target[c][i+target_size,:])\n",
    "        \n",
    "    return np.array(data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "mental-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    mse = mean_squared_error(actual, pred)\n",
    "    msa = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    variance = explained_variance_score(actual, pred)\n",
    "    return mse, msa, r2, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "attended-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_size=20\n",
    "target_size=0\n",
    "\n",
    "X_train, y_train = multivariate_data(train_X, train_y, 0, TRAIN_SPLIT, history_size, target_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "egyptian-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = multivariate_data(test_X, test_y, 0, len(test_y), history_size, target_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "smart-attendance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(504, 20, 40)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "dense-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_set = train_set.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_set = test_set.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "turned-watts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ((None, 20, 40), (None, 2)), types: (tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "driven-apollo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 20, 32)            9344      \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 12,514\n",
      "Trainable params: 12,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "UNIT = 32\n",
    "OPTIMIZER='adam'\n",
    "LOSS='mse'\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(UNIT,return_sequences=True,input_shape=X_train.shape[-2:]),\n",
    "    tf.keras.layers.LSTM(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "focal-consortium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 36s 17ms/step - loss: 0.0775 - val_loss: 7.7637\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 34s 17ms/step - loss: 0.0010 - val_loss: 8.1156\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 33s 16ms/step - loss: 2.3506e-04 - val_loss: 8.9807\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 34s 17ms/step - loss: 1.3677e-04 - val_loss: 7.7681\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 1.4509e-04 - val_loss: 6.9844\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 9.9202e-05 - val_loss: 6.6277\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 1.3491e-04 - val_loss: 6.6259\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 5.5893e-05 - val_loss: 6.4802\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 31s 16ms/step - loss: 4.3298e-04 - val_loss: 6.3210\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 3.2457e-05 - val_loss: 6.3213\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Optimizer=adam:\n",
      "MSE:  6.321309048803643\n",
      "MSA:  2.1001269974007486\n",
      "R-Squared:  -546.0041864798935\n",
      "Explained Variance Score:  -0.1675097321829513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_13_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/m8/zs0d09l904s28pmh_g_t5q640000gn/T/tmpzefa0mdr/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/m8/zs0d09l904s28pmh_g_t5q640000gn/T/tmpzefa0mdr/model/data/model/assets\n"
     ]
    }
   ],
   "source": [
    "EVALUATION_INTERVAL = 2000\n",
    "EPOCHS = 10\n",
    "model_name = 'goodness_LTSM'\n",
    "\n",
    "with mlflow.start_run(run_name=model_name) as run:\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    history = model.fit(train_set, \n",
    "                  epochs=EPOCHS,\n",
    "                  steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                  validation_data=test_set, \n",
    "                  validation_steps=50)\n",
    "    \n",
    "    # Log model, scaler, model parameters to MLflow\n",
    "    mlflow.log_param(\"units\", UNIT)\n",
    "    mlflow.log_param(\"optimizer\", OPTIMIZER)\n",
    "    mlflow.log_param(\"loss\", LOSS)\n",
    "    mlflow.log_param(\"evaluation interval\", EVALUATION_INTERVAL)\n",
    "    mlflow.log_param(\"epochs\", EPOCHS)\n",
    "    mlflow.log_param(\"batch size\", BATCH_SIZE)\n",
    "    \n",
    "    predictions = model.predict(X_test, verbose=1)\n",
    "    # Invert transform on predictions\n",
    "#     predictions = y_scaler.inverse_transform(predictions)\n",
    "    (mse, msa, r2, variance) = eval_metrics(y_test, predictions)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"Optimizer={}:\".format(OPTIMIZER))\n",
    "    print('MSE: ', mse)\n",
    "    print('MSA: ', msa)\n",
    "    print('R-Squared: ', r2)\n",
    "    print('Explained Variance Score: ', variance)\n",
    "    \n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"msa\", msa)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"variance\", variance)\n",
    "\n",
    "    mlflow.keras.log_model(model, model_name)\n",
    "    mlflow.sklearn.log_model(inputScaler, 'InputScaler')\n",
    "    mlflow.sklearn.log_model(outputScaler, 'OutScaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-dialogue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
