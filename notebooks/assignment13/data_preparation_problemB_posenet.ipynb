{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem B (PoseNet)\n",
    "\n",
    "Section for configurations and imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://35.228.45.76:5000')\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='../../keys/mlflow-312506-8cfad529f4fd.json'\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import data augmentation\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from augmentation.methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 47\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices( 'GPU' )\n",
    "print( 'Num GPUs Available: ', len( physical_devices ) )\n",
    "if len( physical_devices ) > 0:\n",
    "    tf.config.experimental.set_memory_growth( physical_devices[0], True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load PoseNet files and classify each sample into good or bad depending on the file name\n",
    "\n",
    "These CSV files were created by our PoseNet model at [test.html](https://github.com/digitacs/4dv652-frontend/blob/main/test.html).<br />\n",
    "W is the start of file names containing a bad exercise, and all others file names are to be classified as good exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'https://raw.githubusercontent.com/digitacs/4dv652-ml/main/datasets/all_good_bad_videos/posenet_good_vs_bad_not_preprocessed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Cut leading and trailing frames\n",
    "\n",
    "When loading the files, we'll use this method for removing leading and trailing frames from each data sample-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_start_scaler = mlflow.sklearn.load_model('gs://mlflow-atlas/mlflow_artifacts/0/ca84e7c5b9e54551bd4708aa457bf730/artifacts/InputScaler')\n",
    "cut_start_model = mlflow.keras.load_model('gs://mlflow-atlas/mlflow_artifacts/0/ca84e7c5b9e54551bd4708aa457bf730/artifacts/cut_start_posenet')\n",
    "\n",
    "cut_stop_scaler = mlflow.sklearn.load_model('gs://mlflow-atlas/mlflow_artifacts/0/583700c9367d4a49ad54912df95cf3cb/artifacts/InputScaler')\n",
    "cut_stop_model = mlflow.keras.load_model('gs://mlflow-atlas/mlflow_artifacts/0/583700c9367d4a49ad54912df95cf3cb/artifacts/cut_stop_posenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_leading_trailing(data):\n",
    "    trimmed_data = data.copy()\n",
    "    scaled_data_start = cut_start_scaler.transform(data, copy=True)\n",
    "\n",
    "    # Remove start frames\n",
    "    predictions = cut_start_model.predict(scaled_data_start)\n",
    "    predictions = predictions.round().astype(int)\n",
    "    \n",
    "    # Find start point based on density of true predictions\n",
    "    surrounding_area = 5\n",
    "    n = 0\n",
    "    s = 0\n",
    "    \n",
    "    for pred in predictions:\n",
    "        if pred == 1:\n",
    "            s += 1\n",
    "        else:\n",
    "            if( s > surrounding_area  ):\n",
    "                break\n",
    "            else:\n",
    "                s = 0\n",
    "        n = n + 1\n",
    "    try:\n",
    "        if n < (len(data) / 2):\n",
    "            trimmed_data = trimmed_data.loc[n:,:]\n",
    "    except:\n",
    "        print('Error trying to remove start frames')\n",
    "\n",
    "    # Remove stop frames\n",
    "    scaled_data_stop = cut_stop_scaler.transform(trimmed_data, copy=True)\n",
    "    predictions = cut_stop_model.predict(scaled_data_stop)\n",
    "    predictions = predictions.round().astype(int)\n",
    "    predictions = np.flip(predictions)\n",
    "\n",
    "    surrounding_area = 5\n",
    "    s = 0\n",
    "    n = len(data)\n",
    "\n",
    "    for pred in predictions:\n",
    "        if pred == 1:\n",
    "            s += 1\n",
    "        else:\n",
    "            if( s > surrounding_area  ):\n",
    "                break\n",
    "            else:\n",
    "                s = 0\n",
    "        n -= 1\n",
    "    try:\n",
    "        if n > (len(data) / 2):\n",
    "            trimmed_data = trimmed_data.loc[:n,:]\n",
    "    except:\n",
    "        print('Error trying to remove stop frames')\n",
    "\n",
    "    return trimmed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find file:  W05.csv\n",
      "Could not find file:  G67.csv\n",
      "Could not find file:  G70.csv\n",
      "(70173, 27)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path + 'A1.csv')\n",
    "# Drop scores-, eye-, and ear-columns\n",
    "df = df[df.columns.drop(list(df.filter(regex='_score')))]\n",
    "df = df[df.columns.drop(list(df.filter(regex='_eye_')))]\n",
    "df = df[df.columns.drop(list(df.filter(regex='_ear_')))]\n",
    "# Rename nose to head in PoseNet data\n",
    "df.rename(columns={'nose_x': 'head_x', 'nose_y': 'head_y'}, inplace=True)\n",
    "df['quality'] = 1 # Good\n",
    "\n",
    "numbers = ['01', '02', '03', '04', '05', '06', '07', '08', '09']\n",
    "\n",
    "for i in numbers:\n",
    "  try:\n",
    "    file_name = 'G{}.csv'.format(i)\n",
    "    temp = pd.read_csv(file_path +  file_name)\n",
    "    # Drop scores-, eye-, and ear-columns\n",
    "    temp = temp[temp.columns.drop(list(temp.filter(regex='_score')))]\n",
    "    temp = temp[temp.columns.drop(list(temp.filter(regex='_eye_')))]\n",
    "    temp = temp[temp.columns.drop(list(temp.filter(regex='_ear_')))]\n",
    "    # Rename nose to head in PoseNet data\n",
    "    temp.rename(columns={'nose_x': 'head_x', 'nose_y': 'head_y'}, inplace=True)\n",
    "    temp = cut_leading_trailing(temp)\n",
    "    temp['quality'] = 1 # Good\n",
    "    df = df.append(temp, ignore_index=True)\n",
    "\n",
    "    file_name = 'W{}.csv'.format(i)\n",
    "    temp = pd.read_csv(file_path +  file_name)\n",
    "    # Drop scores-, eye-, and ear-columns\n",
    "    temp = temp[temp.columns.drop(list(temp.filter(regex='_score')))]\n",
    "    temp = temp[temp.columns.drop(list(temp.filter(regex='_eye_')))]\n",
    "    temp = temp[temp.columns.drop(list(temp.filter(regex='_ear_')))]\n",
    "    # Rename nose to head in PoseNet data\n",
    "    temp.rename(columns={'nose_x': 'head_x', 'nose_y': 'head_y'}, inplace=True)\n",
    "    temp = cut_leading_trailing(temp)\n",
    "    temp['quality'] = 0 # Bad\n",
    "    df = df.append(temp, ignore_index=True)\n",
    "  except IOError as e:\n",
    "    print('Could not find file: ', file_name)\n",
    "\n",
    "for i in range(20, 83):\n",
    "  try:\n",
    "    file_name = 'G{}.csv'.format(i)\n",
    "    temp = pd.read_csv(file_path +  file_name)\n",
    "    # Drop scores-, eye-, and ear-columns\n",
    "    temp = temp[temp.columns.drop(list(temp.filter(regex='_score')))]\n",
    "    temp = temp[temp.columns.drop(list(temp.filter(regex='_eye_')))]\n",
    "    temp = temp[temp.columns.drop(list(temp.filter(regex='_ear_')))]\n",
    "    # Rename nose to head in PoseNet data\n",
    "    temp.rename(columns={'nose_x': 'head_x', 'nose_y': 'head_y'}, inplace=True)\n",
    "    temp = cut_leading_trailing(temp)\n",
    "    temp['quality'] = 1 # Good\n",
    "    df = df.append(temp, ignore_index=True)\n",
    "  except IOError as e:\n",
    "    print('Could not find file: ', file_name)\n",
    "\n",
    "for i in range(10, 44):\n",
    "  try:\n",
    "    file_name = 'W{}.csv'.format(i)\n",
    "    temp = pd.read_csv(file_path +  file_name)\n",
    "    # Drop scores-, eye-, and ear-columns\n",
    "    temp = temp[temp.columns.drop(list(temp.filter(regex='_score')))]\n",
    "    temp = temp[temp.columns.drop(list(temp.filter(regex='_eye_')))]\n",
    "    temp = temp[temp.columns.drop(list(temp.filter(regex='_ear_')))]\n",
    "    # Rename nose to head in PoseNet data\n",
    "    temp.rename(columns={'nose_x': 'head_x', 'nose_y': 'head_y'}, inplace=True)\n",
    "    temp = cut_leading_trailing(temp)\n",
    "    temp['quality'] = 0 # Bad\n",
    "    df = df.append(temp, ignore_index=True)\n",
    "  except IOError as e:\n",
    "    print('Could not find file: ', file_name)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Augmentation\n",
    "\n",
    "## Mirror X coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140346, 27)\n"
     ]
    }
   ],
   "source": [
    "df = mirror(df,'x', append=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch X coordinate by 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280692, 27)\n"
     ]
    }
   ],
   "source": [
    "df_temp = augMultiplier(df.drop(columns=['quality']), multiplier=1.5)\n",
    "df_temp['quality'] = df['quality']\n",
    "df = df.append(df_temp, ignore_index=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress by 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561384, 27)\n"
     ]
    }
   ],
   "source": [
    "df_temp = augMultiplier(df.drop(columns=['quality']), multiplier=0.25)\n",
    "df_temp['quality'] = df['quality']\n",
    "df = df.append(df_temp, ignore_index=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotate by p/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(565384, 27)\n"
     ]
    }
   ],
   "source": [
    "samples = df.sample(2000)\n",
    "\n",
    "angle = 3.1415 / 7\n",
    "samples_rotated = rotate(samples.drop(columns=['quality']), angle=angle, posenet=True)\n",
    "samples_rotated['quality'] = samples['quality'].append(samples['quality'], ignore_index=True)\n",
    "df = df.append(samples_rotated, ignore_index=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotate by -p/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569384, 27)\n"
     ]
    }
   ],
   "source": [
    "samples = df.sample(2000)\n",
    "\n",
    "angle = 3.1415 / -9\n",
    "samples_rotated = rotate(samples.drop(columns=['quality']), angle=angle, posenet=True)\n",
    "samples_rotated['quality'] = samples['quality'].append(samples['quality'], ignore_index=True)\n",
    "df = df.append(samples_rotated, ignore_index=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save as New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_size = 4\n",
    "cut_size = int(len(df) / slice_size)\n",
    "q = 0\n",
    "\n",
    "for i in range(slice_size):\n",
    "    temp = df.loc[cut_size*i:cut_size*(i+1)-1,:]\n",
    "    temp.to_csv('../../datasets/all_good_bad_problemB_posenet/good_bad_posenet_{}.csv'.format(i+1), index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
