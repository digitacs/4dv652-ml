{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qa5Xe4Cl-78"
      },
      "source": [
        "# Assignment 12 - Updated Version\n",
        "\n",
        "*by Angelica Hjelm Gardner, Muhammad Farooq, and Omid Golshan Tafti.*\n",
        "\n",
        "This report is an updated version of the previous submission to fix the points raised in the previous assignments (e.g. including executable code) and an update of the pipeline implementation as we were experiencing server issues before when this assignment was submitted - and those issues are now fixed.\n",
        "\n",
        "## Introduction (this part is kept intact from the first submission)\n",
        "\n",
        "In this sprint, we've implemented a new endpoint to our application that accepts a webcam recording or video upload, and gives back a skeleton avatar animation to the user. <br />\n",
        "This report begins with a user documentation, explaining how to get started with using the application as well as a step-by-step guide on how to use it.\n",
        "It also contains a part for technical documentation where we introduce the three different parts of our application (i.e. ML, API, UI) and focus extra on explaining the updates from this new version.\n",
        "\n",
        "----\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. **User Documentation**\n",
        "\n",
        "    1.1 Get Started\n",
        "\n",
        "    1.2 Step-by Step Guide\n",
        "\n",
        "2. **Technical Documentation**\n",
        "\n",
        "    2.1 ML Updates\n",
        "\n",
        "    2.2 API Updates\n",
        "\n",
        "    2.3 UI Updates\n",
        "\n",
        "    2.4 Current ML Models in the Pipeline\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djqPz0BHN4Px"
      },
      "source": [
        "# 1. User Documentation\n",
        "\n",
        "Our application lets users become more health-conscious by assessing overhead squat movements performed by users either through video recordings or numerical joint positions. The application predicts a score and the weakest link in a user's musculoskeletal system. It also provides a skeleton avatar animation as a visual representation of the user's exercise.\n",
        "\n",
        "This section introduces our application from a user perspective.\n",
        "The first subsection describes how a user can get started with our application, and the second subsection contains a step-by-step guide how to use it. That subsection demonstrates the webcam and video upload functionality where you would receive the skeleton animation. And it will also shows how to use the form to get a score and the prediction for weakest link.\n",
        "\n",
        "## 1.1 Get Started\n",
        "\n",
        "Please use Chrome browser!\n",
        "\n",
        "A user visits our website at [http://rhtrv.com:9000/](http://rhtrv.com:9000/) via a web browser. \n",
        "\n",
        "All functionality will work except for the web camera. To use our web camera implementation, you need to first add our web app domain to be treated as a secure site. This is because currently, we're using the insecure HTTP protocol to serve our web app, and therefore, Chrome will not get permission to access your web camera. <br />\n",
        "We're looking into how to move our web app to a secure connection on our server, but as for now, we don't want to potentially cause any interruptions to the setup, which, in turn, might lead to downtime.\n",
        "\n",
        "## 1.1.1 Add our website as secure in Chrome\n",
        "\n",
        "Open this link in your Chrome browser [chrome://flags/#unsafely-treat-insecure-origin-as-secure](chrome://flags/#unsafely-treat-insecure-origin-as-secure), and add our domain (i.e.http://rhtrv.com:9000/) to \"Insecure origins treated as secure\", as shown below in Figure 1.1.\n",
        "\n",
        "![Figure 1.1](https://drive.google.com/uc?export=view&id=1TZ_lUnAsiC3D0m0JwNaxwpC3qFK9fvBe)\n",
        "\n",
        "<small>Figure 1.1: Add our domain to `Insecure origins treated as secure`.</small>\n",
        "\n",
        "After enabling this setting, visit:  [http://rhtrv.com:9000/](http://rhtrv.com:9000/) and you will be met with three alternatives; each of these are described in the section below.\n",
        "\n",
        "## 1.2 Step-by-Step Guide\n",
        "\n",
        "A step-by-step guide on how to use each part of our application. \n",
        "\n",
        "### 1.2.1 Form submission to get Score and Weakest link\n",
        "\n",
        "The form submission is supposed to be used when the user has performed an overhead squat assessment and received a set of numbers representing certain keypoints of your musculoskeletal system.\n",
        "You find the form submission by pressing the middle alternative at the home page:\n",
        "\n",
        "![Figure 1.2](https://drive.google.com/uc?export=view&id=1Kvigqn85Vc7rDNGxPJSCtn5nPhI2CIT1)\n",
        "\n",
        "When you're located at the form submission page, you'll find three alternatives:\n",
        "\n",
        "1. Fill in each value in separate form submission input fields;\n",
        "2. Place each predictor on a new line in the box (you should not use comma between variables); and\n",
        "3. Place each value on a new line and use comma as the separator.\n",
        "\n",
        "You may select any alternative of your choice. <br />\n",
        "Additionally, we have a test alternative where values are generated randomly for those wanting to test the functionality - but with dummy values. In that case, you press the red button, and values will appear as shown below:\n",
        "\n",
        "![Figure 1.3](https://drive.google.com/uc?export=view&id=1wnFkxqFwJuYZt_B1SyOkDDrRboNgWJr2)\n",
        "\n",
        "Once the form has all values, we press the blue button to receive the score and weakest link, like so: \n",
        "\n",
        "![Figure 1.4](https://drive.google.com/uc?export=view&id=1jmuogzht8i7m4yq-WAlUIve4Jtzu0pjD)\n",
        "\n",
        "### 1.2.2 Using Web camera to get a Skeleton Animation\n",
        "\n",
        "Web camera functionality works only for single person! \n",
        "\n",
        "The web camera functionality is located to the left:\n",
        "\n",
        "![Figure 1.5](https://drive.google.com/uc?export=view&id=1VgApfA80mNAWcs4u4i4AXXuCbFi21XAv)\n",
        "\n",
        "After granting permission for the website to use your web camera, you will see your webcam feed mirrored on the page. Then, you press the **Start** button to the left and perform the exercise. Please stand in a position where the webcam can capture your full body as making this exercise, like so:\n",
        "\n",
        "![Figure 1.6](https://drive.google.com/uc?export=view&id=10btJLlyc3nroJh0DOwtPyDzIMqfxpeIz)\n",
        "\n",
        "After you've finish performing the overhead squat, you press the **Stop** button so that your recording can be processed by our application.\n",
        "\n",
        "### 1.2.3 Video upload\n",
        "\n",
        "Our application accepts all video formats that are playable in the browser, such as .mp4 and formats listed in the figure below:\n",
        "\n",
        "![Figure 1.7](https://drive.google.com/uc?export=view&id=1TZ_lUnAsiC3D0m0JwNaxwpC3qFK9fvBe)\n",
        "\n",
        "<small>Please use one of these formats when uploading videos.</small>\n",
        "\n",
        "To the right on our website, we have the functionality to upload a video recording.\n",
        "\n",
        "![Figure 1.8](https://drive.google.com/uc?export=view&id=1BX5xikUduvPPf9M-rM85N3BOKFZGEnZd)\n",
        "\n",
        "The video recording should be of one person doing a overhead deep squat movement. After uploading the video, you press **Submit** as shown below:\n",
        "\n",
        "![Figure 1.9](https://drive.google.com/uc?export=view&id=1wiygWkeMs2QIidC21gx53PIJMg8GQj5f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF0N43PgUnPN"
      },
      "source": [
        "# 2. Technical Documentation\n",
        "\n",
        "This section describes our application from the technical perspective. \n",
        "We're using three components:\n",
        "\n",
        "- ML component for experimenting, tracking, and serving our ML models;\n",
        "- Backend component consisting of a RESTful API built with Django. It receives requests from the user interface and retrieves predictions from our ML models;\n",
        "- Frontend component, a single-page web application built with Angular that communicates with our backend API.\n",
        "\n",
        "Each of these components will be briefly explained in a subsection below, and we will also introduce our latest updates to each component, which we have worked on this sprint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr41X5hwYxBo"
      },
      "source": [
        "## 2.1 ML Updates\n",
        "\n",
        "This section describes how we have approached ML model experimentation and what we're using to serve the best performing models. \n",
        "\n",
        "### 2.1.1 Model Improvements\n",
        "\n",
        "We're experimenting with our ML models in one Jupyter Notebook per model. These are the notebooks:\n",
        "\n",
        "- [posenet_to_kinect2d.ipynb](https://github.com/digitacs/4dv652-ml/blob/main/notebooks/assignment12/posenet_to_kinect2d.ipynb)\n",
        "- [kinect2d_to_3d.ipynb](https://github.com/digitacs/4dv652-ml/blob/main/notebooks/assignment12/kinect2d_to_3d.ipynb)\n",
        "- [uncut_to_cut.ipynb](https://github.com/digitacs/4dv652-ml/blob/main/notebooks/assignment12/uncut_to_cut.ipynb)\n",
        "\n",
        "We're using [MLFlow Tracking](https://mlflow.org/docs/latest/tracking.html) for logging parameters, code versions, metrics, and output files when running our ML code and up until this sprint, the logging has been done locally. This way, we eliminate the need for manually saving results when comparing experimentation runs as MLflow does that for us. MLFlow works with both Sklearn and Keras models.\n",
        "\n",
        "In each Notebook, we're using the `mlflow` module to start a run where we have provided variables to represent hyperparameters such as batch size, learning rate, optimization and activation function, as well as what results we get for the evaluation metrics we're using.\n",
        "\n",
        "In the Notebooks, we're using a `create_model()` method to create a Sequential model in Keras. We also want the ability to test different layer setups, so we have a variable to represent layers which is a list of Python dictionaries: one dictionary per layer.<br />\n",
        "Then, we're using the Factory pattern to create layer objects. Currently, the factory contains the possibility to experiment with these types of layers:\n",
        "\n",
        "`Conv1D, Dense, MaxPooling1D, Dropout, Flatten`\n",
        "\n",
        "But we can easily extend the alternatives in the Factory, if needed.\n",
        "\n",
        "The model is then compiled using the optimizer and learning rate of choice, as such:\n",
        "\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.get(optimizer)\n",
        "optimizer.learning_rate.assign(learning_rate)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "```\n",
        "\n",
        "And finally, it's returned from the function - ready to be used for fitting.\n",
        "An example of this could look like:\n",
        "\n",
        "```python\n",
        "with mlflow.start_run() as run:\n",
        "    # Model parameters\n",
        "    learning_rate = 0.001\n",
        "    optimizer = 'Adam'\n",
        "    loss = 'mse'\n",
        "    metrics = ['mae']\n",
        "    epochs = 500\n",
        "    batch_size = 16\n",
        "    layers = [ \n",
        "        { 'type': 'Dense', 'nodes':64, 'activation': 'relu' },\n",
        "        { 'type': 'Dense', 'nodes':64, 'activation': 'relu' },\n",
        "        { 'type': 'Dense', 'nodes':26, 'activation': ''}\n",
        "    ]\n",
        "\n",
        "    model = create_model(optimizer=optimizer)\n",
        "    history = model.fit(x=X_train, y=y_train, validation_split=0.2, shuffle=True, epochs=epochs, verbose=1, batch_size=batch_size, callbacks=[early_stopping])\n",
        "\n",
        "    predictions = model.predict(X_test, verbose=1)\n",
        "    # Invert transform on predictions\n",
        "    predictions = y_scaler.inverse_transform(predictions)\n",
        "    (mse, msa, r2, variance) = eval_metrics(y_test, predictions)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\nPoseNet_to_Kinect model (optimizer={}, learning_rate={}):\".format(optimizer, learning_rate))\n",
        "    print('MSE: ', mse)\n",
        "    print('MSA: ', msa)\n",
        "    print('R-Squared: ', r2)\n",
        "    print('Explained Variance Score: ', variance)\n",
        "\n",
        "    # Log parameter, metrics, and model to MLflow\n",
        "    mlflow.log_param(\"optimizer\", optimizer)\n",
        "    mlflow.log_param(\"learning rate\", learning_rate)\n",
        "    mlflow.log_param(\"batch size\", batch_size)\n",
        "    mlflow.log_metric(\"mse\", mse)\n",
        "    mlflow.log_metric(\"msa\", msa)\n",
        "    mlflow.log_metric(\"r2\", r2)\n",
        "    mlflow.log_metric(\"variance\", variance)\n",
        "\n",
        "    mlflow.keras.log_model(model, \"posenet_to_kinect2d\")\n",
        "```\n",
        "\n",
        "Some reflections we noticed when performing these experimentations:\n",
        "\n",
        "- The **Adam** optimizer usually gave the best performance and it preferred smaller batch sizes for our regression problems were the best was a batch size of 16. \n",
        "- Early stopping helped to avoid overfitting for all three models by automatically stopping when the chosen metric (i.e. validation loss - mse, we want the min value - for the regression problems and the validation precision-recall curve - we want the max value - for classification problems) has stopped improving for 10 epochs.\n",
        "\n",
        "#### 2.1.2 ML Dashboard\n",
        "\n",
        "At this sprint, we decided to implement an online dashboard with MLFlow so that we can easily track and compare our experimentation runs together and not only individually/locally. We can also use this online dashboard to serve the best performing ML models. <br />\n",
        "The dashboard can be found at [35.228.45.76:5000](http://35.228.45.76:5000/)\n",
        "Unfortunately, we don’t have any protection on the MLFlow dashboard for now as we had to promitize other parts of the application development, but we're working on adding that asap.\n",
        "\n",
        "We connect to the dashboard in the Notebooks by using the following lines directly after importing the `mlflow` module.\n",
        "\n",
        "```python\n",
        "mlflow.set_tracking_uri('http://35.228.45.76:5000')\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='./mlflow-312506-6387830e8324.json'\n",
        "```\n",
        "\n",
        "The `.json` file contains google cloud credentials to connect to other dashboard service, but this file is not uploaded to the repository so we keep the credentials secure. \n",
        "\n",
        "It's also important that you have Google Cloud and Google Cloud Storage installed, you can use the following command:\n",
        "\n",
        "`pip install google-cloud google-cloud-storage`\n",
        "\n",
        "At the dashboard, we have two tabs, which we can see in the figure below. These are: \n",
        "\n",
        "- An overview for experiments containing the parameters and metrics, among other information; and \n",
        "- An overview of registrered models. Here we can see what version of the model that is in production.\n",
        "\n",
        "![Figure 2.1](https://drive.google.com/uc?export=view&id=1F78XPjQfokAveEA2kGyYyesJeJrjB58V)\n",
        "\n",
        "#### 2.1.3 Serving our ML models\n",
        "\n",
        "When an MLflow Model is created from a run it is logged with Keras' `log_model()` method and we see from the metrics that this was the best performing model, we can register that model at MLflow Model Registry. Once a model is logged, this model can be registered with the Model Registry from an experiment's **Artifacts** section, as seen in the Figure below:\n",
        "\n",
        "![Figure 2.2](https://drive.google.com/uc?export=view&id=10H1Ly9utDznwaGOP1ai9RxMRjuIbIM7b)\n",
        "\n",
        "Then, when navigating to the **Models** tab at the dashboard, we can see details about models and transition their status to Archieved, Staging, or Production. \n",
        "\n",
        "![Figure 2.3](https://drive.google.com/uc?export=view&id=1NlrYavDmvnO-6w1C3YObe1rocnLuyvpX)\n",
        "\n",
        "After we have registered our MLflow models, we can fetch them in our backend API to include them in our application pipeline. We do this by using `mlflow.keras.load_model()` method where we feed the model with input we have received from our application, and use the output for its purpose. This process is described further in section 2.2.1 about the new API endpoint.\n",
        "\n",
        "If we want to compare the experimentation runs (perhaps we want to see how changing some parameter values have affected the model performance), then we can select the models of choice from the dashboard, like so:\n",
        "\n",
        "![Figure 2.4](https://drive.google.com/uc?export=view&id=1z7CnQ5Su0u5ZJSPs1vr9Lnfek4r3Q850)\n",
        "\n",
        "And after clicking on the \"Compare\" button, we will get a page with information logged about all selected experiments and there's also the possibility to create plots, for example this scatter plot below where we can see different R-squared values that the models have reached depending on what optimizer function that was used. If we hover over one of the dots, it will show information about that model.\n",
        "\n",
        "![Figure 2.5](https://drive.google.com/uc?export=view&id=19wY6fNrhMRIGQ8qdhJvxLDoyBrYmgUdY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jshfAchYzWL"
      },
      "source": [
        "## 2.2 API Updates\n",
        "\n",
        "The second component of our application that we're introducing in this technical documentation part is our RESTful API built with Django. <br />\n",
        "In this sprint, we needed to introduce a new endpoint. So at this section, we're taking the opportunity to briefly present the history of our API and how it has involved to support the app functionality as it currently is. \n",
        "\n",
        "All status codes are standard HTTP status codes. The below ones are used in this API:\n",
        "\n",
        "- 2XX - Success of some kind\n",
        "- 4XX - Error occurred in client’s part\n",
        "- 5XX - Error occurred in server’s part\n",
        "\n",
        "| Status Code | Description |\n",
        "| --- | --- |\n",
        "| 200 | OK |\n",
        "| 201 | Created |\n",
        "| 400 | Bad request |\n",
        "| 500 | Internal Server Error |\n",
        "| 503 | Service Unavailable |\n",
        "\n",
        "#### 2.2.1 History of the API Endpoints\n",
        "\n",
        "##### **scores (v1)**\n",
        "\n",
        "This was our first version of the API. <br />\n",
        "The API method (POST) receives an input of numerical coordinates and predicts a score (of goodness for the exercise) using a Linear Regression Model. Only the score was returned.\n",
        "\n",
        "*Content Type*:<br />\n",
        "JSON\n",
        "\n",
        "| Method | URL |\n",
        "| --- | --- |\n",
        "| POST | api/v1/scores/| \n",
        "\n",
        "<br />\n",
        "\n",
        "| Input Parameters | Type |\n",
        "| --- | --- | \n",
        "| No_1_Angle_Deviation | FLOAT |\n",
        "No_2_Angle_Deviation\n",
        "No_3_Angle_Deviation\n",
        "No_4_Angle_Deviation\n",
        "No_5_Angle_Deviation\n",
        "No_6_Angle_Deviation\n",
        "No_7_Angle_Deviation\n",
        "No_8_Angle_Deviation\n",
        "No_9_Angle_Deviation\n",
        "No_10_Angle_Deviation\n",
        "No_11_Angle_Deviation\n",
        "No_12_Angle_Deviation\n",
        "No_13_Angle_Deviation\n",
        "No_1_NASM_Deviation \n",
        "No_2_NASM_Deviation\n",
        "No_3_NASM_Deviation\n",
        "No_4_NASM_Deviation\n",
        "No_5_NASM_Deviation\n",
        "No_6_NASM_Deviation\n",
        "No_7_NASM_Deviation\n",
        "No_8_NASM_Deviation\n",
        "No_9_NASM_Deviation\n",
        "No_10_NASM_Deviation\n",
        "No_11_NASM_Deviation\n",
        "No_12_NASM_Deviation\n",
        "No_13_NASM_Deviation\n",
        "No_14_NASM_Deviation\n",
        "No_15_NASM_Deviation\n",
        "No_16_NASM_Deviation\n",
        "No_17_NASM_Deviation\n",
        "No_18_NASM_Deviation\n",
        "No_19_NASM_Deviation\n",
        "No_20_NASM_Deviation\n",
        "No_21_NASM_Deviation\n",
        "No_22_NASM_Deviation\n",
        "No_23_NASM_Deviation\n",
        "No_24_NASM_Deviation\n",
        "No_25_NASM_Deviation\n",
        "No_1_Time_Deviation\n",
        "No_2_Time_Deviation\n",
        "\n",
        "<br />\n",
        "\n",
        "| Output | Type |\n",
        "| --- | --- |\n",
        "| score | FLOAT |\n",
        "\n",
        "##### **scores (v2)**\n",
        "\n",
        "This was our second version of the API. <br />\n",
        "The API method (POST) receives the same input of numerical coordinates as before, and now predicts a score (of goodness for the exercise) using a Linear Regression Model and the weakest link in the musculoskeletal system using a Logistic Regression model. Both the score and the weakest link is returned.\n",
        "\n",
        "The content type and input parameters are the same as before. \n",
        "\n",
        "| Method | URL |\n",
        "| --- | --- |\n",
        "| POST | api/v2/scores/| \n",
        "\n",
        "<br />\n",
        "\n",
        "| Output | Type |\n",
        "| --- | --- |\n",
        "| score | FLOAT |\n",
        "| weakest_link | String |\n",
        "\n",
        "##### **videoupload (v3)** \n",
        "\n",
        "The videoupload endpoint was added during this week, and it was supposed to receive a video recording uploaded from the UI that we would use PoseNet model at our backend to process. <br />\n",
        "As it turned out, our server did not support the use of Tensorflow and OpenCV (required for PoseNet) once we tried to deploy that solution - so we ended up switching to PoseNet at the frontend. Therefore, we're currently not using this endpoint with the deployed application in use. \n",
        "\n",
        "##### **camupload (v3)** \n",
        "\n",
        "The camupload endpoint was also added during this week, and it was supposed to support the webcam functionality. The webcam uses PoseNet at the frontend and sends its output to the backend for processing. And because we weren't able to use PoseNet at the backend, we're now processing video uploads at the frontend and also send PoseNet output from videos to this endpoint - so it's used by both webcam recordings and video upload functionalities. We will need to add a task to clean up this and remove any unnecessary code/endpoint in a later maintenance sprint. \n",
        "\n",
        "The current version of the API is v3. The scores endpoint is still in use for returning the score and weakest link, and now we have this endpoint to process the video upload or camera recordings and return a skeleton animation avatar.\n",
        "\n",
        "*Content Type*:<br />\n",
        "JSON\n",
        "\n",
        "| Method | URL |\n",
        "| --- | --- |\n",
        "| POST | api/v3/camupload/| \n",
        "\n",
        "<br />\n",
        "\n",
        "| Input Parameters | Type |\n",
        "| --- | --- |\n",
        "| Frames List | List | \n",
        "\n",
        "<br /> \n",
        "\n",
        "| Output | Type | \n",
        "| --- | --- | \n",
        "| file | String | \n",
        "\n",
        "#### 2.2.2 Connecting to MLflow\n",
        "\n",
        "Our Django API receives coordinates from PoseNet looking similar to this truncated example:\n",
        "\n",
        "```json\n",
        "{\"frames\":\n",
        "[[\n",
        "  {\"score\":0.9970989227294922,\"part\":\"nose\",\"position\":{\"x\":461.9463021323983,\"y\":133.34247247718187}},\n",
        "  {\"score\":0.9797642230987549,\"part\":\"leftEye\",\"position\":{\"x\":454.2126104469854,\"y\":125.83175154511568}},\n",
        "  {\"score\":0.996979832649231,\"part\":\"rightEye\",\"position\":{\"x\":471.04795377418054,\"y\":126.88906806916115}},\n",
        "  {\"score\":0.5669078230857849,\"part\":\"leftEar\",\"position\":{\"x\":444.7451548744883,\"y\":135.51062379829614}},\n",
        "  {\"score\":0.9454719424247742,\"part\":\"rightEar\",\"position\":{\"x\":482.0735604331796,\"y\":135.6166504143741}},\n",
        "]]\n",
        "}\n",
        "```\n",
        "\n",
        "With this input, it connects to our ML models that are served with MLflow. First, we load the posenet_to_kinect model, then kinect2d_to_3d, and lastly cutting leading and trailing frames. All models are loaded in the following way:\n",
        "\n",
        "```python\n",
        "kinect3D_model = mlflow.keras.load_model('gs://mlflow-atlas/mlflow_artifacts/0/cbca4a49c97a4f0a9e100a90658a5cb6/artifacts/kinect3D')\n",
        "predictions = kinect3D_model.predict(d)\n",
        "return Response({'file':serializer}, status=HTTP_200_OK)\n",
        "```\n",
        "\n",
        "The full implementation can be seen in [views.py](https://github.com/digitacs/4dv652-backend/blob/main/scores/views.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCXQxz9JagsD"
      },
      "source": [
        "## 2.3 UI Updates\n",
        "\n",
        "This section describes how our web application in Angular is built, especially focusing on the latest updates from this sprint. To get started with using this component, please see our [README](https://github.com/digitacs/4dv652-frontend/blob/main/README.md) at the repository. \n",
        "\n",
        "#### 2.3.1 Uploading a Video\n",
        "\n",
        "At our front page, we have an Angular component ([OptionCardComponent](https://github.com/digitacs/4dv652-frontend/tree/main/src/app/modules/dashboard/option-card)) handling redirect of the three main functionalities our app supports to each responsible component:\n",
        "\n",
        "- [ParameterFeedComponent](https://github.com/digitacs/4dv652-frontend/tree/main/src/app/modules/dashboard/parameter-feed) (form submission)\n",
        "- [LiveFeedComponent](https://github.com/digitacs/4dv652-frontend/tree/main/src/app/modules/dashboard/live-feed) (webcam)\n",
        "- [UploadVideoComponent](https://github.com/digitacs/4dv652-frontend/tree/main/src/app/modules/dashboard/upload-video) (added this sprint to support video upload)\n",
        "\n",
        "UploadVideoComponent is using an upload service that we created. Here' we're using Angular's FormData with file upload.\n",
        "\n",
        "#### 2.3.2 PoseNet\n",
        "\n",
        "As mentioned in section 2.2.1 about API endpoints, our first idea was to use PoseNet at the backend with Python, but as we were deploying that solution to our server, we noticed that it could not support the use of Tensorflow and OpenCV, so we had to rethink and use PoseNet at the frontend with Tensorflow.js. As we're already using PoseNet at the frontend for the webcam functionality, we could also included the video processing part. So these are two functionalities with slightly different user experience, but most of the appliation processing is the same. <br />\n",
        "The full implementation of using PoseNet for video upload can be seen in [upload-video.component.ts](https://github.com/digitacs/4dv652-frontend/blob/main/src/app/modules/dashboard/upload-video/upload-video.component.ts)\n",
        "\n",
        "#### 2.3.3 Displaying the Skeleton Avatar Animation\n",
        "\n",
        "For the webcam and video upload functionalities, we process the recordings with PoseNet, as mentioned.<br />\n",
        "After processing a video, we send our PoseNet data to the Django API and in response we will receive 3D predictions for each frame. From this response, we want to create a skeleton avatar animation for the user, so, we draw each element (i.e. head, left_shoulder, right_shoulder, etc.) as vertex showing their connection as a line in an HTML canvas element.\n",
        "\n",
        "The result is an animation from 3 different point of views, showing the user's overhead squat movement after it has been:\n",
        "\n",
        "- Processed by PoseNet in 2D;\n",
        "- Imitated to match a Kinect 2D device;\n",
        "- A third dimension is added for 3D;\n",
        "- Leading and trailing video frames are removed, so the end results only displays the actual exercise. \n",
        "\n",
        "An example of the output avatar can be seen in the code block below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpiz9v5SPAys"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"https://github.com/digitacs/4dv652-frontend/blob/main/videos/skeleton_animation.mov?raw=true\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuDpxSoyPq3F"
      },
      "source": [
        "## 2.4 Current ML Models in the Pipeline\n",
        "\n",
        "We'll look quickly at the ML models that create this pipeline. \n",
        "\n",
        "### 2.4.1 First model: PoseNet\n",
        "\n",
        "First is PoseNet using Tensorflow.js working with webcam and video upload on  the frontend. This model is served by our [PoseNet service](https://github.com/digitacs/4dv652-frontend/blob/main/src/app/modules/utils/posenet.service.ts) in Angular.\n",
        "\n",
        "### 2.4.2 Second model: Cut Start frames (Dense model)\n",
        "\n",
        "PoseNet provides its keypoint coordinates to the Cut Start frames model. This model cuts PoseNet Start frames so that after this process, any frames that were not part of the exercise from the start of the recording has (hopefully) been removed. The Cut Start model looks like the following:\n",
        "\n",
        "This is the dataset we used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oR96E7v3kWC"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbMMtBlcS-9Y"
      },
      "source": [
        "data_path = 'https://raw.githubusercontent.com/digitacs/4dv652-ml/main/datasets/new_posenet_marked_start_end/'\n",
        "\n",
        "df = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX6cD-AqTOmD"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# A-files\n",
        "for i in range (1,160):\n",
        "  try:\n",
        "    dataset = pd.read_csv(data_path + 'A{}.csv'.format(i))\n",
        "    dataset = dataset[dataset.columns.drop(list(dataset.filter(regex='_score')))]\n",
        "    dataset = dataset[dataset.columns.drop(list(dataset.filter(regex='_eye_')))]\n",
        "    dataset = dataset[dataset.columns.drop(list(dataset.filter(regex='_ear_')))]\n",
        "    dataset.rename(columns={'nose_x': 'head_x', 'nose_y': 'head_y'}, inplace=True)\n",
        "\n",
        "    if df is None:\n",
        "      df = dataset\n",
        "    else:\n",
        "      df = pd.concat((df, dataset), ignore_index=True)\n",
        "\n",
        "  except IOError as e:\n",
        "    print('Error in reading file: A{}.csv'.format(i), e)\n",
        "\n",
        "# B-files\n",
        "for i in range(1, 23):\n",
        "  try:\n",
        "    dataset = pd.read_csv(data_path + 'B{}.csv'.format(i))\n",
        "    dataset = dataset[dataset.columns.drop(list(dataset.filter(regex='_score')))]\n",
        "    dataset = dataset[dataset.columns.drop(list(dataset.filter(regex='_eye_')))]\n",
        "    dataset = dataset[dataset.columns.drop(list(dataset.filter(regex='_ear_')))]\n",
        "    dataset.rename(columns={'nose_x': 'head_x', 'nose_y': 'head_y'}, inplace=True)\n",
        "\n",
        "    if df is None:\n",
        "      df = dataset\n",
        "    else:\n",
        "      df = pd.concat((df, dataset), ignore_index=True)\n",
        "\n",
        "  except IOError as e:\n",
        "    print('Error in reading file: B{}.csv'.format(i), e)\n",
        "\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWXAXhqtTWBy"
      },
      "source": [
        "We used some data augmentation to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA3A7WAGVayH"
      },
      "source": [
        "def mirror(data, axis, append=False):\n",
        "    try:\n",
        "        \n",
        "        if axis == 'a':\n",
        "            target_labels = [col for col in data.columns]\n",
        "        else:\n",
        "            axis = \"_\" + axis\n",
        "            target_labels = [col for col in data.columns if axis in col]\n",
        "\n",
        "        aug_data_mirror = data.copy()\n",
        "\n",
        "        for t in target_labels:\n",
        "            temp = -aug_data_mirror[t]\n",
        "            aug_data_mirror = aug_data_mirror.assign(**{t: temp.values})\n",
        "\n",
        "        if append:\n",
        "            return data.append(aug_data_mirror,ignore_index=True)\n",
        "\n",
        "        return aug_data_mirror\n",
        "    \n",
        "    except IOError as e:\n",
        "        print(e)\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSRL8vxbTX4A"
      },
      "source": [
        "df = mirror(df,'x', append=True)\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGI_EJr-Voy9"
      },
      "source": [
        "X = df.drop(columns=['start', 'end'])\n",
        "y = df['start']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmXON0ONTaji"
      },
      "source": [
        "And performed oversampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2GlT_V5Td99"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "random_state=42\n",
        "np.random.seed(random_state)\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "sm = SMOTE(random_state=random_state) # Perform oversamling using SMOTE\n",
        "resampled_X, resampled_y = sm.fit_resample(X, y)\n",
        "\n",
        "print(resampled_X.shape)\n",
        "print(resampled_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa9KuVbhTgf4"
      },
      "source": [
        "Then we split the data into sets for training, validation, and testing - and normalized it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3HDp1chTgOc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(resampled_X, resampled_y, test_size=0.2, random_state=random_state)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=random_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9M9J3mRTmsO"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print('Training labels shape:', y_train.shape)\n",
        "print('Validation labels shape:', y_val.shape)\n",
        "print('Test labels shape:', y_test.shape, '\\n')\n",
        "\n",
        "print('Training features shape:', X_train.shape)\n",
        "print('Validation features shape:', X_val.shape)\n",
        "print('Test features shape:', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygs73_IpVTyB"
      },
      "source": [
        "The model architecture looks like the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHukh5JVWMKr"
      },
      "source": [
        "input_dim = X_train.shape[1]\n",
        "output_size = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpgqblkfVS_P"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "METRICS = [\n",
        "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    tf.keras.metrics.Precision(name='precision'),\n",
        "    tf.keras.metrics.Recall(name='recall'),\n",
        "    tf.keras.metrics.AUC(name='auc'),\n",
        "    tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n",
        "\n",
        "model = Sequential([\n",
        "  InputLayer(input_shape=(input_dim))\n",
        "])\n",
        "model.add(Dense(units=64, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(units=64, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(units=64, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(units=32, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(units=32, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(units=output_size, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWHWciJ9Wykr"
      },
      "source": [
        "We will use early stopping to prevent overfitting, watching the validation loss so it's kept at its minimum. Training this model looks like the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XpjA-lXW1Mj"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='min',\n",
        "    restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWcRfvbuW3ar"
      },
      "source": [
        "history = model.fit(\n",
        "    x=X_train, \n",
        "    y=y_train, \n",
        "    validation_data=(X_val, y_val), \n",
        "    shuffle=True, \n",
        "    epochs=EPOCHS, \n",
        "    verbose=1,\n",
        "    batch_size=BATCH_SIZE, \n",
        "    callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Is9ANyKXBfR"
      },
      "source": [
        "And this is the performance of our pipeline's Cut Start frames model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgSt7Q8dX-wc"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def eval_metrics(actual, pred):\n",
        "    accuracy = accuracy_score(actual, pred)\n",
        "    precision = precision_score(actual, pred)\n",
        "    recall = recall_score(actual, pred)\n",
        "    f1 = f1_score(actual, pred)\n",
        "    return accuracy, precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxd0mf-PXD7_"
      },
      "source": [
        "predictions = model.predict_classes(X_test)\n",
        "\n",
        "accuracy, precision, recall, f1 = eval_metrics(y_test, predictions)\n",
        "\n",
        "print('\\nAccuracy:', accuracy, '\\n')\n",
        "print('Precision score:', precision, '\\n')\n",
        "print('Recall score:', recall, '\\n')\n",
        "print('F1 score:', f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VucE6_72LTz"
      },
      "source": [
        "This model is served via MLflow Model Registry and can be found at [this URL](http://35.228.45.76:5000/#/experiments/0/runs/ca84e7c5b9e54551bd4708aa457bf730)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsTFaQ_4XMId"
      },
      "source": [
        "### 2.4.3 Third model: Cut Stop frames (Dense model)\n",
        "\n",
        "The remaining frames that was not removed by the Cut Start model is passed to the next model which is Cut Stop frames. This model cuts PoseNet Stop frames and that its processing, any frames that were not part of the exercise at the end of the recording has (hopefully) been removed. The remaining frames at the end should only contain the actual overhead deepsquat exercise. \n",
        "\n",
        "Let's look at the Cut Stop frames model. <br />\n",
        "The dataset used is the same as the Cut Start model used just before so we simply need to reassign the X and y values to keep the \"end\" column instead of \"start\" which was used previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x37qZVNKXOkz"
      },
      "source": [
        "X = df.drop(columns=['end', 'start'])\n",
        "y = df['end']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e1Tbvux3cqJ"
      },
      "source": [
        "sm = SMOTE(random_state=random_state) # Perform oversamling using SMOTE\n",
        "resampled_X, resampled_y = sm.fit_resample(X, y)\n",
        "\n",
        "print(resampled_X.shape)\n",
        "print(resampled_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-7x2c2-3fXv"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(resampled_X, resampled_y, test_size=0.2, random_state=random_state)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=random_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT1Rpje23qZh"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print('Training labels shape:', y_train.shape)\n",
        "print('Validation labels shape:', y_val.shape)\n",
        "print('Test labels shape:', y_test.shape, '\\n')\n",
        "\n",
        "print('Training features shape:', X_train.shape)\n",
        "print('Validation features shape:', X_val.shape)\n",
        "print('Test features shape:', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhOsF2Tg3vI9"
      },
      "source": [
        "The model architecture for the Cut Stop frames is the same as Cut Start model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL6Dce_R3yl5"
      },
      "source": [
        "model = Sequential([\n",
        "  InputLayer(input_shape=(input_dim))\n",
        "])\n",
        "model.add(Dense(units=64, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(units=64, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(units=64, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(units=32, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(units=32, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(units=output_size, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZksmjPr4TiS"
      },
      "source": [
        "history = model.fit(\n",
        "    x=X_train, \n",
        "    y=y_train, \n",
        "    validation_data=(X_val, y_val), \n",
        "    shuffle=True, \n",
        "    epochs=EPOCHS, \n",
        "    verbose=1,\n",
        "    batch_size=BATCH_SIZE, \n",
        "    callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATMqABnN4WJl"
      },
      "source": [
        "predictions = model.predict_classes(X_test)\n",
        "\n",
        "accuracy, precision, recall, f1 = eval_metrics(y_test, predictions)\n",
        "\n",
        "print('\\nAccuracy:', accuracy, '\\n')\n",
        "print('Precision score:', precision, '\\n')\n",
        "print('Recall score:', recall, '\\n')\n",
        "print('F1 score:', f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgv-DAGI7m9D"
      },
      "source": [
        "This model is also served via MLflow Model Registry and can be found at [this URL](http://35.228.45.76:5000/#/experiments/0/runs/583700c9367d4a49ad54912df95cf3cb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH3lEGpxRySs"
      },
      "source": [
        "### 2.4.4 Fourth model: PoseNet to Kinect 2D (Dense model)\n",
        "\n",
        "After start and stop frames has been removed, the data is transferred to the next model in the pipeline which is PoseNet to Kinect 2D.\n",
        "\n",
        "Lets load the dataset we've used for this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkFX3ec1XVtH"
      },
      "source": [
        "posenetDataPath = 'https://raw.githubusercontent.com/digitacs/4dv652-ml/main/datasets/posenet-uncut/old_without_score/'\n",
        "kinectDataPath = 'https://raw.githubusercontent.com/digitacs/4dv652-ml/main/datasets/kinect_fixed_not_cut_dup_fNo/'\n",
        "\n",
        "X = None\n",
        "y = None\n",
        "\n",
        "train_test_ratio = 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcR3Q8g4-JrQ"
      },
      "source": [
        "import re\n",
        "\n",
        "# A-files\n",
        "for i in range(1,160):\n",
        "  try:\n",
        "    posenetData = pd.read_csv(posenetDataPath + 'A{}.csv'.format(i))\n",
        "    scores = []\n",
        "    for score in posenetData.columns:\n",
        "      if re.search(\"^.*_score$\", score):\n",
        "        scores.append(score)\n",
        "    posenetData.drop(columns=scores, inplace=True)\n",
        "    posenetData.drop(columns=['left_eye_x', 'left_eye_y', 'right_eye_x', 'right_eye_y', 'left_ear_x', 'left_ear_y', 'right_ear_x', 'right_ear_y'], inplace=True)\n",
        "    posenetData.rename(columns={'nose_x': 'head_x', 'nose_y': 'head_y'}, inplace=True)\n",
        "\n",
        "    kinectData = pd.read_csv(kinectDataPath + 'A{}_kinect.csv'.format(i))\n",
        "    # Check amount of frames\n",
        "    if (len(posenetData) == len(kinectData)):\n",
        "      if X is None:\n",
        "        X = posenetData\n",
        "      else:\n",
        "        X = pd.concat((X, posenetData), ignore_index=True)\n",
        "\n",
        "    # Drop Z-columns from Kinect\n",
        "    z = []\n",
        "    for c in kinectData.columns:\n",
        "      if re.search(\"^.*_z$\", c):\n",
        "        z.append(c)\n",
        "    kinectData.drop(columns=z, inplace=True)\n",
        "    kinectData.drop(columns=['Unnamed: 0', 'FrameNo'], inplace=True)\n",
        "    if y is None:\n",
        "      y = kinectData\n",
        "    else:\n",
        "      y = pd.concat((y, kinectData), ignore_index=True)\n",
        "  except IOError as e:\n",
        "    print('Error in reading file: {}'.format(i), e)\n",
        "\n",
        "# B-files\n",
        "for i in range(1, 23):\n",
        "  try:\n",
        "    posenetData = pd.read_csv(posenetDataPath + 'B{}.csv'.format(i))\n",
        "    scores = []\n",
        "    for score in posenetData.columns:\n",
        "      if re.search(\"^.*_score$\", score):\n",
        "        scores.append(score)\n",
        "    posenetData.drop(columns=scores, inplace=True)\n",
        "    posenetData.drop(columns=['left_eye_x', 'left_eye_y', 'right_eye_x', 'right_eye_y', 'left_ear_x', 'left_ear_y', 'right_ear_x', 'right_ear_y'], inplace=True)\n",
        "    posenetData.rename(columns={'nose_x': 'head_x', 'nose_y': 'head_y'}, inplace=True)\n",
        "\n",
        "    kinectData = pd.read_csv(kinectDataPath + 'B{}_kinect.csv'.format(i))\n",
        "    # Check amount of frames\n",
        "    if (len(posenetData) == len(kinectData)):\n",
        "      if X is None:\n",
        "        X = posenetData\n",
        "      else:\n",
        "        X = pd.concat((X, posenetData), ignore_index=True)\n",
        "\n",
        "    # Drop Z-columns from Kinect\n",
        "    z = []\n",
        "    for c in kinectData.columns:\n",
        "      if re.search(\"^.*_z$\", c):\n",
        "        z.append(c)\n",
        "    kinectData.drop(columns=z, inplace=True)\n",
        "    kinectData.drop(columns=['Unnamed: 0', 'FrameNo'], inplace=True)\n",
        "    if y is None:\n",
        "      y = kinectData\n",
        "    else:\n",
        "      y = pd.concat((y, kinectData), ignore_index=True)\n",
        "  except IOError as e:\n",
        "    print('Error in reading file: {}'.format(i), e)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O37noS--ILl"
      },
      "source": [
        "X_train, X_test, y_train, y_test  = train_test_split(X, y, train_size=train_test_ratio, random_state=random_state)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJs1jEV3-NU1"
      },
      "source": [
        "We scale both the features and labels as this is a regression problem. Then we use inverse scaling for labels before comparing the predictions with the real labels in *y_test*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9_-BEla-M0z"
      },
      "source": [
        "X_scaler = StandardScaler()\n",
        "X_train = X_scaler.fit_transform(X_train)\n",
        "X_test = X_scaler.transform(X_test)\n",
        "\n",
        "y_scaler = StandardScaler()\n",
        "y_train = y_scaler.fit_transform(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex9t6tqzCvy3"
      },
      "source": [
        "\n",
        "The model variant for PoseNet to Kinect 2D model is also Dense and it's architecture looks like the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmIIXGbCC048"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "METRICS = [\n",
        "    tf.keras.metrics.MeanSquaredError(name=\"mse\", dtype=None),\n",
        "    tf.keras.metrics.MeanAbsoluteError(name=\"mae\", dtype=None),\n",
        "    tf.keras.metrics.RootMeanSquaredError(name=\"rmse\", dtype=None),\n",
        "]\n",
        "\n",
        "model = Sequential([\n",
        "  InputLayer(input_shape=(input_dim))\n",
        "])\n",
        "model.add(Dense(units=64, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=32, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=output_size))\n",
        "\n",
        "model.compile(optimizer=RMSprop(learning_rate=1e-3), loss='mse', metrics=METRICS)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8ojVH3iDumt"
      },
      "source": [
        "We need to change the early stopping criteria to fit our regression problem and therefore we choose lowest loss as the critera."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58eMQistCxEz"
      },
      "source": [
        "BATCH_SIZE = 56  \n",
        "\n",
        "history = model.fit(\n",
        "    x=X_train, \n",
        "    y=y_train, \n",
        "    validation_split=0.2, \n",
        "    shuffle=True, \n",
        "    epochs=EPOCHS, \n",
        "    verbose=1,\n",
        "    batch_size=BATCH_SIZE, \n",
        "    callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MxaOCyJSs_Q"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error\n",
        "\n",
        "def eval_metrics(actual, pred):\n",
        "  mse = mean_squared_error(actual, pred)\n",
        "  msa = mean_absolute_error(actual, pred)\n",
        "  r2 = r2_score(actual, pred)\n",
        "  variance = explained_variance_score(actual, pred)\n",
        "  return mse, msa, r2, variance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2dtjPEcDCV1"
      },
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "predictions = y_scaler.inverse_transform(predictions) \n",
        "(mse, msa, r2, variance) = eval_metrics(y_test, predictions)\n",
        " \n",
        "print('\\nMSE: ', mse, '\\n')\n",
        "print('MSA: ', msa, '\\n')\n",
        "print('R-Squared: ', r2, '\\n')\n",
        "print('Explained Variance Score: ', variance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qkAK-Kf9pJe"
      },
      "source": [
        "Serving this model is done with MLflow and it is found [here](http://35.228.45.76:5000/#/experiments/0/runs/33d92199595745e3a005bb31f620c839)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fiXZrN_R8TI"
      },
      "source": [
        "### 2.4.5 Fifth model: Kinect 2D to 3D (Dense model)\n",
        "\n",
        "At this stage of the application pipeline, we have exercise frames in Kinect 2D. They will not be converted into 3D using our Kinect 2D to 3D model. \n",
        "\n",
        "We're using another dataset to train this model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uYnLc4yXPE-"
      },
      "source": [
        "import re\n",
        "\n",
        "def read_from_file( file_name ):\n",
        "    data = pd.read_csv('https://raw.githubusercontent.com/digitacs/4dv652-ml/main/datasets/kinect_good_preprocessed/{}.csv'.format( file_name ))\n",
        "    data = data.drop( columns=['FrameNo'] )\n",
        "\n",
        "    target_labels = []\n",
        "    for c in data.columns:\n",
        "        if re.search(\"^.*_z$\", c):\n",
        "            target_labels.append(c)\n",
        "\n",
        "    target_data = data[target_labels]\n",
        "    input_data = data.drop( columns=target_labels )\n",
        "    \n",
        "    return input_data, target_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9VKQFuw93mz"
      },
      "source": [
        "# Generate 10 random numbers\n",
        "X, y = read_from_file('A1_kinect')\n",
        "files = random.sample(range(2, 151), 10)\n",
        "\n",
        "# Retrieve 10 training sequences\n",
        "for i in files:\n",
        "    try:\n",
        "        iData, tData = read_from_file('A{}_kinect'.format(i))\n",
        "        X = np.concatenate((X, iData))\n",
        "        y = np.concatenate((y, tData))\n",
        "    except IOError as e:\n",
        "        print('Error in reading A{}_kinect.csv'.format(i),e)\n",
        "\n",
        "X, y = shuffle(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iALdRmAqUFyf"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=random_state)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_size = y_train.shape[1]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2L8LU7AUHhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad1b4d4-8e95-4e60-9c58-4b9b056119e5"
      },
      "source": [
        "X_scaler = StandardScaler()\n",
        "X_train = X_scaler.fit_transform(X_train)\n",
        "\n",
        "X_val = X_scaler.transform(X_val)\n",
        "X_test = X_scaler.transform(X_test)\n",
        "\n",
        "y_scaler = StandardScaler()\n",
        "y_train = y_scaler.fit_transform(y_train)\n",
        "y_val = y_scaler.transform(y_val)\n",
        "\n",
        "print('Training features shape:', X_train.shape)\n",
        "print('Training labels shape:', y_train.shape, '\\n')\n",
        "\n",
        "print('Validation features shape:', X_val.shape)\n",
        "print('Validation labels shape:', y_val.shape, '\\n')\n",
        "\n",
        "print('Test features shape:', X_test.shape)\n",
        "print('Test labels shape:', y_test.shape, '\\n')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training features shape: (80, 26)\n",
            "Training labels shape: (80, 13) \n",
            "\n",
            "Validation features shape: (20, 26)\n",
            "Validation labels shape: (20, 13) \n",
            "\n",
            "Test features shape: (12, 26)\n",
            "Test labels shape: (12, 13) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JrVIcuQUfA6"
      },
      "source": [
        "The model architecture for Kinect 2D to 3D model looks like the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERj0iLyVUR6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5201c784-023d-4f9b-fe2e-d2563a59f6e3"
      },
      "source": [
        "model = Sequential([\n",
        "  InputLayer(input_shape=(input_dim))\n",
        "])\n",
        "model.add(Dense(units=38, activation='relu'))\n",
        "model.add(Dense(units=38, activation='relu'))\n",
        "model.add(Dense(units=38, activation='relu'))\n",
        "model.add(Dense(units=output_size, activation='linear'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-2), loss='mse', metrics=METRICS)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_25 (Dense)             (None, 38)                1026      \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 38)                1482      \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 38)                1482      \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 13)                507       \n",
            "=================================================================\n",
            "Total params: 4,497\n",
            "Trainable params: 4,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQVaq3l0Vv6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dddf97f2-b212-45c9-95d5-04078163a795"
      },
      "source": [
        "history = model.fit(\n",
        "    x=X_train, \n",
        "    y=y_train, \n",
        "    validation_split=0.2, \n",
        "    shuffle=True, \n",
        "    epochs=EPOCHS, \n",
        "    verbose=1,\n",
        "    batch_size=BATCH_SIZE, \n",
        "    callbacks=[early_stopping])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 153ms/step - loss: 1.0265 - mse: 0.1048 - mae: 0.1601 - rmse: 0.3218 - val_loss: 0.6457 - val_mse: 0.6457 - val_mae: 0.6653 - val_rmse: 0.8036\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6470 - mse: 0.6470 - mae: 0.6854 - rmse: 0.8044 - val_loss: 0.5138 - val_mse: 0.5138 - val_mae: 0.5987 - val_rmse: 0.7168\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5089 - mse: 0.5089 - mae: 0.5965 - rmse: 0.7134 - val_loss: 0.3674 - val_mse: 0.3674 - val_mae: 0.4776 - val_rmse: 0.6061\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.3539 - mse: 0.3539 - mae: 0.4743 - rmse: 0.5949 - val_loss: 0.2905 - val_mse: 0.2905 - val_mae: 0.4210 - val_rmse: 0.5390\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2473 - mse: 0.2473 - mae: 0.4008 - rmse: 0.4973 - val_loss: 0.2098 - val_mse: 0.2098 - val_mae: 0.3596 - val_rmse: 0.4580\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1760 - mse: 0.1760 - mae: 0.3393 - rmse: 0.4196 - val_loss: 0.1676 - val_mse: 0.1676 - val_mae: 0.3177 - val_rmse: 0.4095\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.1523 - mse: 0.1523 - mae: 0.3050 - rmse: 0.3903 - val_loss: 0.1309 - val_mse: 0.1309 - val_mae: 0.2804 - val_rmse: 0.3618\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.1138 - mse: 0.1138 - mae: 0.2631 - rmse: 0.3373 - val_loss: 0.1169 - val_mse: 0.1169 - val_mae: 0.2753 - val_rmse: 0.3419\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0854 - mse: 0.0854 - mae: 0.2313 - rmse: 0.2922 - val_loss: 0.1116 - val_mse: 0.1116 - val_mae: 0.2687 - val_rmse: 0.3340\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0795 - mse: 0.0795 - mae: 0.2236 - rmse: 0.2819 - val_loss: 0.0940 - val_mse: 0.0940 - val_mae: 0.2336 - val_rmse: 0.3066\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0703 - mse: 0.0703 - mae: 0.2096 - rmse: 0.2652 - val_loss: 0.0883 - val_mse: 0.0883 - val_mae: 0.2287 - val_rmse: 0.2971\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0689 - mse: 0.0689 - mae: 0.2090 - rmse: 0.2625 - val_loss: 0.0813 - val_mse: 0.0813 - val_mae: 0.2063 - val_rmse: 0.2852\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0634 - mse: 0.0634 - mae: 0.1918 - rmse: 0.2518 - val_loss: 0.0654 - val_mse: 0.0654 - val_mae: 0.1927 - val_rmse: 0.2558\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0579 - mse: 0.0579 - mae: 0.1837 - rmse: 0.2406 - val_loss: 0.0526 - val_mse: 0.0526 - val_mae: 0.1771 - val_rmse: 0.2294\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0448 - mse: 0.0448 - mae: 0.1640 - rmse: 0.2117 - val_loss: 0.0813 - val_mse: 0.0813 - val_mae: 0.2130 - val_rmse: 0.2851\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.0574 - mse: 0.0574 - mae: 0.1809 - rmse: 0.2396 - val_loss: 0.0624 - val_mse: 0.0624 - val_mae: 0.1864 - val_rmse: 0.2498\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0459 - mse: 0.0459 - mae: 0.1647 - rmse: 0.2143 - val_loss: 0.0449 - val_mse: 0.0449 - val_mae: 0.1641 - val_rmse: 0.2120\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0337 - mse: 0.0337 - mae: 0.1425 - rmse: 0.1836 - val_loss: 0.0474 - val_mse: 0.0474 - val_mae: 0.1728 - val_rmse: 0.2176\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0372 - mse: 0.0372 - mae: 0.1521 - rmse: 0.1929 - val_loss: 0.0382 - val_mse: 0.0382 - val_mae: 0.1530 - val_rmse: 0.1953\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0289 - mse: 0.0289 - mae: 0.1331 - rmse: 0.1701 - val_loss: 0.0375 - val_mse: 0.0375 - val_mae: 0.1489 - val_rmse: 0.1936\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0296 - mse: 0.0296 - mae: 0.1362 - rmse: 0.1720 - val_loss: 0.0298 - val_mse: 0.0298 - val_mae: 0.1294 - val_rmse: 0.1727\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1154 - rmse: 0.1520 - val_loss: 0.0289 - val_mse: 0.0289 - val_mae: 0.1322 - val_rmse: 0.1701\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1187 - rmse: 0.1519 - val_loss: 0.0278 - val_mse: 0.0278 - val_mae: 0.1243 - val_rmse: 0.1668\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1067 - rmse: 0.1413 - val_loss: 0.0278 - val_mse: 0.0278 - val_mae: 0.1257 - val_rmse: 0.1668\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1093 - rmse: 0.1406 - val_loss: 0.0231 - val_mse: 0.0231 - val_mae: 0.1174 - val_rmse: 0.1519\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0171 - mse: 0.0171 - mae: 0.1029 - rmse: 0.1309 - val_loss: 0.0210 - val_mse: 0.0210 - val_mae: 0.1111 - val_rmse: 0.1450\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0157 - mse: 0.0157 - mae: 0.0973 - rmse: 0.1251 - val_loss: 0.0214 - val_mse: 0.0214 - val_mae: 0.1153 - val_rmse: 0.1463\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0145 - mse: 0.0145 - mae: 0.0944 - rmse: 0.1206 - val_loss: 0.0228 - val_mse: 0.0228 - val_mae: 0.1200 - val_rmse: 0.1510\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0153 - mse: 0.0153 - mae: 0.0980 - rmse: 0.1235 - val_loss: 0.0183 - val_mse: 0.0183 - val_mae: 0.1086 - val_rmse: 0.1353\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0890 - rmse: 0.1135 - val_loss: 0.0177 - val_mse: 0.0177 - val_mae: 0.1049 - val_rmse: 0.1329\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0136 - mse: 0.0136 - mae: 0.0914 - rmse: 0.1164 - val_loss: 0.0156 - val_mse: 0.0156 - val_mae: 0.0993 - val_rmse: 0.1247\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0113 - mse: 0.0113 - mae: 0.0826 - rmse: 0.1064 - val_loss: 0.0150 - val_mse: 0.0150 - val_mae: 0.0955 - val_rmse: 0.1226\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0851 - rmse: 0.1098 - val_loss: 0.0131 - val_mse: 0.0131 - val_mae: 0.0878 - val_rmse: 0.1146\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0780 - rmse: 0.1021 - val_loss: 0.0147 - val_mse: 0.0147 - val_mae: 0.0921 - val_rmse: 0.1214\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0116 - mse: 0.0116 - mae: 0.0820 - rmse: 0.1079 - val_loss: 0.0164 - val_mse: 0.0164 - val_mae: 0.0976 - val_rmse: 0.1282\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0775 - rmse: 0.1021 - val_loss: 0.0166 - val_mse: 0.0166 - val_mae: 0.0987 - val_rmse: 0.1289\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0113 - mse: 0.0113 - mae: 0.0818 - rmse: 0.1062 - val_loss: 0.0152 - val_mse: 0.0152 - val_mae: 0.0944 - val_rmse: 0.1232\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0746 - rmse: 0.0969 - val_loss: 0.0142 - val_mse: 0.0142 - val_mae: 0.0851 - val_rmse: 0.1192\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0741 - rmse: 0.0960 - val_loss: 0.0139 - val_mse: 0.0139 - val_mae: 0.0842 - val_rmse: 0.1179\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0081 - mse: 0.0081 - mae: 0.0669 - rmse: 0.0901 - val_loss: 0.0142 - val_mse: 0.0142 - val_mae: 0.0874 - val_rmse: 0.1190\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0729 - rmse: 0.0946 - val_loss: 0.0127 - val_mse: 0.0127 - val_mae: 0.0824 - val_rmse: 0.1125\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0648 - rmse: 0.0851 - val_loss: 0.0138 - val_mse: 0.0138 - val_mae: 0.0826 - val_rmse: 0.1177\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0089 - mse: 0.0089 - mae: 0.0693 - rmse: 0.0942 - val_loss: 0.0107 - val_mse: 0.0107 - val_mae: 0.0765 - val_rmse: 0.1037\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0634 - rmse: 0.0855 - val_loss: 0.0143 - val_mse: 0.0143 - val_mae: 0.0903 - val_rmse: 0.1195\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0093 - mse: 0.0093 - mae: 0.0721 - rmse: 0.0967 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0717 - val_rmse: 0.1007\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0584 - rmse: 0.0785 - val_loss: 0.0107 - val_mse: 0.0107 - val_mae: 0.0746 - val_rmse: 0.1034\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0596 - rmse: 0.0814 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0713 - val_rmse: 0.0982\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0063 - mse: 0.0063 - mae: 0.0577 - rmse: 0.0794 - val_loss: 0.0106 - val_mse: 0.0106 - val_mae: 0.0778 - val_rmse: 0.1032\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0604 - rmse: 0.0818 - val_loss: 0.0112 - val_mse: 0.0112 - val_mae: 0.0753 - val_rmse: 0.1059\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0594 - rmse: 0.0809 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0712 - val_rmse: 0.0998\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0553 - rmse: 0.0749 - val_loss: 0.0103 - val_mse: 0.0103 - val_mae: 0.0736 - val_rmse: 0.1016\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0592 - rmse: 0.0797 - val_loss: 0.0082 - val_mse: 0.0082 - val_mae: 0.0630 - val_rmse: 0.0906\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0496 - rmse: 0.0688 - val_loss: 0.0120 - val_mse: 0.0120 - val_mae: 0.0782 - val_rmse: 0.1094\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0572 - rmse: 0.0807 - val_loss: 0.0087 - val_mse: 0.0087 - val_mae: 0.0652 - val_rmse: 0.0930\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0504 - rmse: 0.0704 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0675 - val_rmse: 0.0942\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0492 - rmse: 0.0684 - val_loss: 0.0123 - val_mse: 0.0123 - val_mae: 0.0850 - val_rmse: 0.1108\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0611 - rmse: 0.0833 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0670 - val_rmse: 0.0911\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0518 - rmse: 0.0704 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0748 - val_rmse: 0.0990\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0568 - rmse: 0.0747 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0695 - val_rmse: 0.0965\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0517 - rmse: 0.0703 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0645 - val_rmse: 0.0909\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0467 - rmse: 0.0650 - val_loss: 0.0083 - val_mse: 0.0083 - val_mae: 0.0676 - val_rmse: 0.0911\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0482 - rmse: 0.0652 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0614 - val_rmse: 0.0871\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0429 - rmse: 0.0587 - val_loss: 0.0076 - val_mse: 0.0076 - val_mae: 0.0617 - val_rmse: 0.0872\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0035 - mse: 0.0035 - mae: 0.0423 - rmse: 0.0592 - val_loss: 0.0069 - val_mse: 0.0069 - val_mae: 0.0601 - val_rmse: 0.0828\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0418 - rmse: 0.0569 - val_loss: 0.0062 - val_mse: 0.0062 - val_mae: 0.0575 - val_rmse: 0.0785\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0422 - rmse: 0.0561 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0522 - val_rmse: 0.0735\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0396 - rmse: 0.0543 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0532 - val_rmse: 0.0721\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0386 - rmse: 0.0532 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0532 - val_rmse: 0.0724\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0030 - mse: 0.0030 - mae: 0.0396 - rmse: 0.0543 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0496 - val_rmse: 0.0704\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0026 - mse: 0.0026 - mae: 0.0385 - rmse: 0.0513 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0486 - val_rmse: 0.0721\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0398 - rmse: 0.0531 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0487 - val_rmse: 0.0705\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0025 - mse: 0.0025 - mae: 0.0371 - rmse: 0.0504 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0498 - val_rmse: 0.0683\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - mae: 0.0375 - rmse: 0.0501 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0513 - val_rmse: 0.0685\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0364 - rmse: 0.0483 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0512 - val_rmse: 0.0707\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0361 - rmse: 0.0480 - val_loss: 0.0045 - val_mse: 0.0045 - val_mae: 0.0474 - val_rmse: 0.0671\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0022 - mse: 0.0022 - mae: 0.0354 - rmse: 0.0470 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0468 - val_rmse: 0.0661\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0022 - mse: 0.0022 - mae: 0.0363 - rmse: 0.0473 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0445 - val_rmse: 0.0610\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0018 - mse: 0.0018 - mae: 0.0325 - rmse: 0.0425 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0515 - val_rmse: 0.0718\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0025 - mse: 0.0025 - mae: 0.0358 - rmse: 0.0498 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0480 - val_rmse: 0.0656\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0360 - rmse: 0.0476 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0466 - val_rmse: 0.0638\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0020 - mse: 0.0020 - mae: 0.0346 - rmse: 0.0448 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0478 - val_rmse: 0.0666\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0020 - mse: 0.0020 - mae: 0.0340 - rmse: 0.0452 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0434 - val_rmse: 0.0615\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0018 - mse: 0.0018 - mae: 0.0322 - rmse: 0.0422 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0453 - val_rmse: 0.0637\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0021 - mse: 0.0021 - mae: 0.0346 - rmse: 0.0456 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0452 - val_rmse: 0.0618\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0018 - mse: 0.0018 - mae: 0.0327 - rmse: 0.0430 - val_loss: 0.0031 - val_mse: 0.0031 - val_mae: 0.0392 - val_rmse: 0.0554\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0313 - rmse: 0.0414 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0408 - val_rmse: 0.0581\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0021 - mse: 0.0021 - mae: 0.0343 - rmse: 0.0459 - val_loss: 0.0039 - val_mse: 0.0039 - val_mae: 0.0437 - val_rmse: 0.0627\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0019 - mse: 0.0019 - mae: 0.0322 - rmse: 0.0440 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0431 - val_rmse: 0.0582\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.0020 - mse: 0.0020 - mae: 0.0345 - rmse: 0.0449 - val_loss: 0.0041 - val_mse: 0.0041 - val_mae: 0.0487 - val_rmse: 0.0642\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0368 - rmse: 0.0476 - val_loss: 0.0044 - val_mse: 0.0044 - val_mae: 0.0477 - val_rmse: 0.0665\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0029 - mse: 0.0029 - mae: 0.0408 - rmse: 0.0539 - val_loss: 0.0036 - val_mse: 0.0036 - val_mae: 0.0422 - val_rmse: 0.0604\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0025 - mse: 0.0025 - mae: 0.0399 - rmse: 0.0503 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0423 - val_rmse: 0.0616\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0019 - mse: 0.0019 - mae: 0.0324 - rmse: 0.0437 - val_loss: 0.0030 - val_mse: 0.0030 - val_mae: 0.0409 - val_rmse: 0.0551\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0016 - mse: 0.0016 - mae: 0.0303 - rmse: 0.0399 - val_loss: 0.0033 - val_mse: 0.0033 - val_mae: 0.0460 - val_rmse: 0.0577\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0018 - mse: 0.0018 - mae: 0.0335 - rmse: 0.0419 - val_loss: 0.0038 - val_mse: 0.0038 - val_mae: 0.0422 - val_rmse: 0.0613\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0019 - mae: 0.0323 - rmse: 0.0441 - val_loss: 0.0032 - val_mse: 0.0032 - val_mae: 0.0415 - val_rmse: 0.0562\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0321 - rmse: 0.0409 - val_loss: 0.0028 - val_mse: 0.0028 - val_mae: 0.0398 - val_rmse: 0.0525\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0287 - rmse: 0.0369 - val_loss: 0.0028 - val_mse: 0.0028 - val_mae: 0.0394 - val_rmse: 0.0530\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0271 - rmse: 0.0358 - val_loss: 0.0032 - val_mse: 0.0032 - val_mae: 0.0429 - val_rmse: 0.0566\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0016 - mse: 0.0016 - mae: 0.0319 - rmse: 0.0399 - val_loss: 0.0027 - val_mse: 0.0027 - val_mae: 0.0351 - val_rmse: 0.0516\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0292 - rmse: 0.0383 - val_loss: 0.0023 - val_mse: 0.0023 - val_mae: 0.0338 - val_rmse: 0.0482\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0256 - rmse: 0.0335 - val_loss: 0.0032 - val_mse: 0.0032 - val_mae: 0.0430 - val_rmse: 0.0569\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0323 - rmse: 0.0412 - val_loss: 0.0027 - val_mse: 0.0027 - val_mae: 0.0401 - val_rmse: 0.0518\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0305 - rmse: 0.0388 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0447 - val_rmse: 0.0580\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0310 - rmse: 0.0409 - val_loss: 0.0033 - val_mse: 0.0033 - val_mae: 0.0443 - val_rmse: 0.0577\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.0021 - mse: 0.0021 - mae: 0.0364 - rmse: 0.0461 - val_loss: 0.0034 - val_mse: 0.0034 - val_mae: 0.0430 - val_rmse: 0.0580\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0016 - mse: 0.0016 - mae: 0.0313 - rmse: 0.0399 - val_loss: 0.0029 - val_mse: 0.0029 - val_mae: 0.0423 - val_rmse: 0.0540\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0018 - mse: 0.0018 - mae: 0.0339 - rmse: 0.0424 - val_loss: 0.0029 - val_mse: 0.0029 - val_mae: 0.0409 - val_rmse: 0.0541\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0321 - rmse: 0.0417 - val_loss: 0.0025 - val_mse: 0.0025 - val_mae: 0.0355 - val_rmse: 0.0499\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0300 - rmse: 0.0383 - val_loss: 0.0027 - val_mse: 0.0027 - val_mae: 0.0389 - val_rmse: 0.0522\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0286 - rmse: 0.0383 - val_loss: 0.0025 - val_mse: 0.0025 - val_mae: 0.0376 - val_rmse: 0.0497\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00111: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRK0AetKPEZq",
        "outputId": "38afa6d3-9c54-433a-98c0-57d5d40d196d"
      },
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "predictions = y_scaler.inverse_transform(predictions) \n",
        "(mse, msa, r2, variance) = eval_metrics(y_test, predictions)\n",
        " \n",
        "print('\\nMSE: ', mse, '\\n')\n",
        "print('MSA: ', msa, '\\n')\n",
        "print('R-Squared: ', r2, '\\n')\n",
        "print('Explained Variance Score: ', variance)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MSE:  2.0091628949748786e-06 \n",
            "\n",
            "MSA:  0.00105208787278148 \n",
            "\n",
            "R-Squared:  0.9986626620802401 \n",
            "\n",
            "Explained Variance Score:  0.9989202366244625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4LqI_sGSPVs"
      },
      "source": [
        "This final model is served with MLflow and can be found in the Registry [here](http://35.228.45.76:5000/#/experiments/0/runs/47d7fdb1a3ed4ca48159e94ce1d6cbbc).\n",
        "\n",
        "Those were all models we're currently using in our application pipeline. The majority part of this application is handled on the backend, with the exception of PoseNet and displaying the Skeleton to the user."
      ]
    }
  ]
}